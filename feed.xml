<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://miintto.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://miintto.github.io/" rel="alternate" type="text/html" /><updated>2024-09-01T08:02:12+00:00</updated><id>https://miintto.github.io/feed.xml</id><title type="html">miintto.log</title><subtitle>미민또의 프로그래밍</subtitle><author><name>miintto</name></author><entry><title type="html">[번역] Lazy Import와 Cinder가 머신러닝 성능을 이끌어낸 방법</title><link href="https://miintto.github.io/docs/python-ml-lazy-import" rel="alternate" type="text/html" title="[번역] Lazy Import와 Cinder가 머신러닝 성능을 이끌어낸 방법" /><published>2024-08-13T00:00:00+00:00</published><updated>2024-08-13T00:00:00+00:00</updated><id>https://miintto.github.io/docs/python-ml-lazy-import</id><content type="html" xml:base="https://miintto.github.io/docs/python-ml-lazy-import"><![CDATA[<blockquote>
  <p>해당 포스트는 Meta Engineering 블로그의 <a href="https://engineering.fb.com/2024/01/18/developer-tools/lazy-imports-cinder-machine-learning-meta/">Lazy is the new fast: How Lazy Imports and Cinder accelerate machine learning at Meta</a> 포스트를 번역한 글입니다.</p>

  <p>게시일: 2024.01.18</p>
</blockquote>

<h1 id="느릿한-것이-또-다른-빠름이다-lazy-import와-cinder가-머신러닝-성능을-이끌어낸-방법">느릿한 것이 또 다른 빠름이다: Lazy Import와 Cinder가 머신러닝 성능을 이끌어낸 방법</h1>

<p>머신러닝 세계에서 시간은 곧 생명입니다.
머신러닝 모델이 초기 학습 데이터를 처리하는 과정에서 소요되는 1밀리초의 차이는 생산성과 실험 과정에 아주 극명한 차이를 가져올 수 있습니다.</p>

<p>메타에서는 <a href="https://peps.python.org/pep-0690/">Lazy Imports</a> 방식과 파이썬 런타임 <a href="https://github.com/facebookincubator/cinder">Cinder</a>를 도입하면서 모델의 학습 시간을 개선했을 뿐 아니라 전반적인 개발자 경험(DevX)도 축적할 수 있었습니다.</p>

<h2 id="첫-배치-시간-이슈">첫 배치 시간 이슈</h2>

<p>배치 프로세싱 기법은 머신러닝의 판도를 바꾸어 놓았습니다.
덕분에 많은 양의 데이터를 그룹(또는 배치) 단위로 다루면서 모델을 학습시키고 파라미터를 최적화하며 추론 과정을 보다 신속하고 효과적으로 수행할 수 있게 되었습니다.</p>

<p>하지만 머신러닝 작업은 느려 터지기로 악명이 높습니다.
우리는 배치 처리 속도를 개선하기 위해서 TTFB(Time to first batch, 첫 배치 시간)에 주목했습니다.
TTFB란 머신러닝 모델을 학습시키기 위해 “시작” 버튼을 누른 후 첫 번째 배치 데이터가 모델이 입력되기까지의 경과 시간을 의미합니다.
해당 값은 모델의 학습이 진행되는 속도를 결정하는 핵심적인 지표로, 인프라의 오버헤드나 스케줄링 지연과 같은 여러 요인에 따라 천차만별일 수 있습니다.
결국 TTFB를 줄인다는 건 엔지니어에게 지루할 수 있는 개발 대기 시간 단축을 의미하게 되며, 이러한 대기 시간이 길어질수록 많은 리소스가 낭비될 수 있습니다.</p>

<p>좀 더 짧은 TTFB를 위해 메타는 이러한 오버헤드를 줄이는 것을 목표로 삼았고, Lazy Imports와 Cinder가 새로운 해결책으로 부상했습니다.</p>

<h2 id="lazy-imports의-마법">Lazy Imports의 마법</h2>

<p>이전까지 머신러닝 개발자들은 표준 라이브러리 <code class="language-plaintext highlighter-rouge">importlib</code>의 <code class="language-plaintext highlighter-rouge">LazyLoader</code>나 <code class="language-plaintext highlighter-rouge">lazy-import</code> 같은 대안을 모색하며 명시적으로 import 작업을 늦추었습니다.
이러한 접근법은 유용해 보였지만 적용할 수 있는 범위가 좁아서 제한적이었고, 매번 어떤 의존성을 지연시킬지 선택해야 했으며, 그나마도 어떤 경우에는 성능이 좋지도 않았습니다.
이 방식을 계속 가져가려면 세심한 코드베이스 관리와 상당한 양의 코드 리팩토링이 필요했습니다.</p>

<p>반면 <a href="/docs/meta-cinder-lazy-import">Cinder의 Lazy Imports</a>의 접근 방식은 이러한 라이브러리의 한계를 뛰어넘고 개발자 경험을 크게 향상시키는 포괄적이면서도 과감한 전략입니다.
지연시킬 패키지를 수기로 일일이 선택하는 대신, Cinder는 기본적으로 모든 import 작업을 늦추어서 시작 프로세스를 경량화하고 가속하였으며, 결과적으로 패키지가 필요한 순간까지 더 광범위하고 강력하게 지연됩니다. 
이 방법을 사용하면 개발자는 import 패키지 선택의 굴레에서 벗어날 수 있습니다.
또한 typing 전용 작업이나 <code class="language-plaintext highlighter-rouge">TYPE_CHECKING</code> 과정도 필요 없어집니다.
파일 시작 부분에서 <code class="language-plaintext highlighter-rouge">from __future__ import annotations</code>를 명시함으로써 타입 평가를 지연시키고, Lazy Imports는 해당 모듈이 실제로 필요한 부분까지 import 작업을 늦추게 됩니다.
이러한 최적화를 결합하면 비용이 많이 드는 런타임 import를 줄이고 개발 작업이 더욱 간소화됩니다.</p>

<p>Lazy Imports 방식의 효과는 다음과 같습니다.
Meta는 머신러닝 개발을 개선하기 위해 Lazy Imports와 Cinder를 여러 작업에 도입하였으며, 여기에는 우리의 머신러닝 프레임워크와 Jupyter 커널도 포함되어 있습니다.
그 결과, 매우 빠른 시작 시간, 향상된 실험 능력, 감소한 인프라 오버헤드, 그리고 유지 보수하기 간편한 코드를 얻을 수 있었습니다.
우리는 메타의 주요 인공지능 작업에서 TTFB값이 최대 40%까지 개선되는 등 눈부신 성과를 공유할 수 있어서 벅차게 생각하고 있습니다.
이렇게 절약한 시간은 매 실행마다 몇 초에서 몇 분 단위로 다양하게 나타날 수 있습니다.</p>

<p>이렇게 놀라운 결과는 머신러닝 개발자가 모델 학습 단계에 좀 더 빠르게 도달할 수 있다는 의미로 머신러닝 작업의 효율성을 크게 향상시킵니다.</p>

<h2 id="lazy-imports-채택">Lazy Imports 채택</h2>

<p>Lazy Imports를 사용하면서 머신러닝 개발 과정을 상당히 개선하였지만, 처음부터 모든 것이 순탄하지만은 않았습니다.
우리의 의지와 창의성을 시험하는 몇 가지 어려움이 있었습니다.</p>

<h3 id="호환성">호환성</h3>

<p>가장 고심했던 부분은 기존 라이브러리와 Lazy Imports 간의 호환성이었습니다.
PyTorch나 Numba, Numpy, SciPy 혹은 다른 라이브러리는 모듈을 지연시켜서 불러오는 방식과는 잘 어우러지지 않았습니다.
이런 라이브러리는 import 사이드 이펙트 및 Lazy Imports와 잘 작동하지 않는 다른 패턴의 영향을 받았습니다.
구체적으로 파이썬이 모듈을 가져오는 순서가 바뀌거나 늦춰질 수 있어서 클래스나 함수 및 연산 작업이 올바르게 등록되지 않는 상황이 종종 발생했습니다.
이런 상황을 해결하기 위해 import 사이클과 어긋난 부분을 식별하고 처리하는 데 많은 공을 들여야 했습니다.</p>

<h3 id="성능과-신뢰성-사이의-균형">성능과 신뢰성 사이의 균형</h3>

<p>우리는 또한 성능 최적화와 코드 신뢰성 사이에서 타협해야만 했습니다.
Lazy Imports가 TTFB를 꽤 줄여주었고 리소스 투입량을 개선한 건 자명하지만, 파이썬 라이브러리를 불러오는 구문에서 상당한 의미론적 변화가 이루어졌기 때문에 코드베이스가 덜 직관적으로 보일 수 있습니다.
두 가치가 완벽하게 조화를 이루도록 지속적으로 고려하였고, 철저하게 테스트 가능한 부분으로만 의미 변화의 영향을 제한하면서 신뢰성을 보장하였습니다.</p>

<p>기존 코드베이스와 원활한 상호작용을 보장하려면 세심한 검증과 조정이 필요합니다.
복합적이고 다방면에 걸친 머신러닝 모델을 다루는 경우에는 일이 더 복잡해졌는데, 지연된 import의 영향을 철저하게 고려해야만 했습니다.
결론적으로 우리는 Lazy Imports를 초기 및 준비 단계에서 활성화하고 첫 배치가 시작되기 전에 종료되도록 하였습니다.</p>

<h3 id="러닝-커브">러닝 커브</h3>

<p>Lazy Imports와 같은 새로운 패러다임을 적용하다 보면 개발팀에게 러닝 커브가 발생할 수 있습니다.
머신러닝, 인프라 및 시스템 엔지니어들이 새로운 방식에 숙달하면서 구문의 뉘앙스를 이해하고 효과적으로 구현하는 건 그 자체로 일이 됩니다.</p>

<h2 id="메타에서-lazy-imports의-미래">메타에서 Lazy Imports의 미래</h2>

<p>Lazy Imports와 Cinder를 도입함으로써 메타의 인공지능 핵심 작업에서 의미 있는 발전이 나타났습니다.
때때로 기복이 있어 보였지만 궁극적으로 Lazy Imports 방식이 머신러닝의 새로운 혁신을 일으킬 것임을 보여주었습니다.
TTFB의 성공, 개발자들의 경험 개선, 커널 시간 단축은 이러한 출발의 가시적인 결과입니다.
Lazy Imports를 사용하면서 메타의 머신러닝 개발자들은 효율적으로 작업하고, 더 빠르게 시도하며, 신속한 결과를 얻을 수 있는 준비가 되어있습니다.</p>

<p>우리가 Lazy Imports를 적용하면서 좋은 성과를 거두었지만, 여전히 아직 갈 길이 멉니다.
앞으로 우리의 행보는 어떻게 될까요?
아래에 우리가 신경 쓰고 있는 몇 가지 부분을 맛보기로 보여드리겠습니다.</p>

<h3 id="개발자-온보딩-효율화">개발자 온보딩 효율화</h3>

<p>Lazy Imports에 대한 러닝 커브는 새로 합류한 사람들에게 장벽이 될 수 있습니다.
우리는 개발자들이 이러한 혁신적인 방식을 쉽게 수용할 수 있도록 교육 리소스와 온보딩 자료에 신경 쓰고 있습니다.</p>

<h3 id="개선된-도구">개선된 도구</h3>

<p>모듈 import가 지연된 상태에서는 코드 디버깅이 복잡해질 수 있습니다.
우리는 디버깅 프로세스를 간소화하는 도구나 기술을 제작하여 개발자들이 문제를 쉽게 발견하고 해결할 수 있도록 노력하고 있습니다.</p>

<h3 id="커뮤니티와-협력">커뮤니티와 협력</h3>

<p>Lazy Imports의 진정한 힘은 유연성과 다재다능함에 있습니다.
우리는 파이썬 커뮤니티와 협력하여 통찰력과 모범 사례를 공유하고 함께 문제를 해결하고자 합니다.
Lazy Imports와 맞물리는 패러다임과 패턴을 지지하는 견고한 커뮤니티를 구축하는 것이 우리의 미래 우선 과제 중 하나입니다.</p>

<hr />

<h1 id="lazy-is-the-new-fast-how-lazy-imports-and-cinder-accelerate-machine-learning-at-meta">Lazy is the new fast: How Lazy Imports and Cinder accelerate machine learning at Meta</h1>

<p>Time is of the essence in the realm of machine learning (ML) development.
The milliseconds it takes for an ML model to transition from conceptualization to processing the initial training data can dramatically impact productivity and experimentation.</p>

<p>At Meta, we’ve been able to significantly improve our model training times, as well as our overall developer experience (DevX) by adopting <a href="https://peps.python.org/pep-0690/">Lazy Imports</a> and the <a href="https://github.com/facebookincubator/cinder">Python Cinder runtime</a>.</p>

<h2 id="the-time-to-first-batch-challenge">The time to first batch challenge</h2>

<p>Batch processing has been a game changer in ML development.
It handles large volumes of data in groups (or batches) and allows us to train models, optimize parameters, and perform inference more effectively and swiftly.</p>

<p>But ML training workloads are notorious for their sluggish starts.
When we look to improve our batch processing speeds, time to first batch (TTFB) comes into focus.
TTFB is the time elapsed from the moment you hit the “start” button on your ML model training to the point when the first batch of data enters the model for processing.
It is a critical metric that determines the speed at which an ML model goes from idle to learning.
TTFB can vary widely due to factors like infrastructure overhead and scheduling delays.
But reducing TTFB means reducing the development waiting times that can often feel like an eternity to engineers – waiting periods that can quickly amass as expensive resource wastage.</p>

<p>In the pursuit of faster TTFB, Meta set its sights on reducing this overhead, and Lazy Imports with Cinder emerged as a promising solution.</p>

<h2 id="the-magic-of-lazy-imports">The magic of Lazy Imports</h2>

<p>Previously, ML developers explored alternatives like the standard <code class="language-plaintext highlighter-rouge">LazyLoader</code> in <code class="language-plaintext highlighter-rouge">importlib</code> or lazy-import`, to defer explicit imports until necessary.
While promising, these approaches are limited by their much narrower scope, and the need to manually select which dependencies will be lazily imported (often with suboptimal results).
Using these approaches demands meticulous codebase curation and a fair amount of code refactoring.</p>

<p>In contrast, <a href="https://developers.facebook.com/blog/post/2022/06/15/python-lazy-imports-with-cinder/">Cinder’s Lazy Imports</a> approach is a comprehensive and aggressive strategy that goes beyond the limitations of other libraries and delivers significant enhancements to the developer experience.
Instead of painstakingly handpicking imports to become lazy, Cinder simplifies and accelerates the startup process by transparently deferring all imports as a default action, resulting in a much broader and more powerful deferral of imports until the exact moment they’re needed.
Once in place, this method ensures that developers no longer have to navigate the maze of selective import choices.
With it, developers can bid farewell to the need of typing-only imports and the use of <code class="language-plaintext highlighter-rouge">TYPE_CHECKING</code>.
It allows a simple <code class="language-plaintext highlighter-rouge">from __future__ import annotations</code> declaration at the beginning of a file to delay type evaluation, while Lazy Imports defer the actual import statements until required.
The combined effect of these optimizations reduced costly runtime imports and further streamlined the development workflow.</p>

<p>The Lazy Imports solution delivers.
Meta’s initiative to enhance ML development has involved rolling out Cinder with Lazy Imports to several workloads, including our ML frameworks and Jupyter kernels, producing lightning-fast startup times, improved experimentation capabilities, reduced infrastructure overhead, and code that is a breeze to maintain.
We’re pleased to share that Meta’s key AI workloads have experienced noteworthy improvements, with TTFB wins reaching up to 40 percent.
Resulting time savings can vary from seconds to minutes per run.</p>

<p>These impressive results translate to a substantial boost in the efficiency of ML workflows, since they mean ML developers can get to the model training phase more swiftly.</p>

<h2 id="the-challenges-of-adopting-lazy-imports">The challenges of adopting Lazy Imports</h2>

<p>While Lazy Imports’ approach significantly improved ML development, it was not all a bed of roses.
We encountered several hurdles that tested our resolve and creativity.</p>

<h3 id="compatibility">Compatibility</h3>

<p>One of the primary challenges we grappled with was the compatibility of existing libraries with Lazy Imports.
Libraries such as PyTorch, Numba, NumPy, and SciPy, among others, did not seamlessly align with the deferred module loading approach.
These libraries often rely on import side effects and other patterns that do not play well with Lazy Imports.
The order in which Python imports could change or be postponed, often led to side effects failing to register classes, functions, and operations correctly.
This required painstaking troubleshooting to identify and address import cycles and discrepancies.</p>

<h3 id="balancing-performance-versus-dependability">Balancing performance versus dependability</h3>

<p>We also had to strike the right balance between performance optimization and code dependability.
While Lazy Imports significantly reduced TTFB and enhanced resource utilization, it also introduced a considerable semantic change in the way Python imports work that could make the codebase less intuitive.
Achieving the perfect equilibrium was a constant consideration, and was ensured by limiting the impact of semantic changes to only the relevant parts that could be thoroughly tested.</p>

<p>Ensuring seamless interaction with the existing codebase required meticulous testing and adjustments.
The task was particularly intricate when dealing with complex, multifaceted ML models, where the implications of deferred imports needed to be thoroughly considered.
We ultimately opted for enabling Lazy Imports only during the startup and preparation phases and disabling it before the first batch started.</p>

<h3 id="learning-curve">Learning curve</h3>

<p>Adopting new paradigms like Lazy Imports can introduce a learning curve for the development team.
Training ML engineers, infra engineers, and system engineers to adapt to the new approach, understand its nuances, and implement it effectively is a process in itself.</p>

<h2 id="what-is-next-for-lazy-imports-at-meta">What is next for Lazy Imports at Meta?</h2>

<p>The adoption of Lazy Imports and Cinder represented a meaningful enhancement in Meta’s AI key workloads.
It came with its share of ups and downs, but ultimately demonstrated that Lazy Imports can be a game changer in expediting ML development.
The TTFB wins, DevX improvements, and reduced kernel startup times are all tangible results of this initiative.
With Lazy Imports, Meta’s ML developers are now equipped to work more efficiently, experiment more rapidly, and achieve results faster.</p>

<p>While we’ve achieved remarkable success with the adoption of Lazy Imports, our journey is far from over.
So, what’s next for us?
Here’s a glimpse into our future endeavors:</p>

<h3 id="streamlining-developer-onboarding">Streamlining developer onboarding</h3>
<p>The learning curve associated with Lazy Imports can be a challenge for newcomers.
We’re investing in educational resources and onboarding materials to make it easier for developers to embrace this game-changing approach.</p>

<h3 id="enhancing-tooling">Enhancing tooling</h3>

<p>Debugging code with deferred imports can be intricate.
We’re working on developing tools and techniques that simplify the debugging and troubleshooting process, ensuring that developers can quickly identify and resolve issues.</p>

<h3 id="community-collaboration">Community collaboration</h3>

<p>The power of Lazy Imports lies in its adaptability and versatility.
We’re eager to collaborate with the Python community – sharing insights, best practices, and addressing challenges together.
Building a robust community that helps supporting paradigms and patterns that play well with Lazy Imports is one of our future priorities.</p>

<hr />

<p>References</p>

<ul>
  <li><a href="https://engineering.fb.com/2024/01/18/developer-tools/lazy-imports-cinder-machine-learning-meta/">Lazy is the new fast: How Lazy Imports and Cinder accelerate machine learning at Meta - Engineering at Meta</a></li>
</ul>]]></content><author><name>miintto</name></author><category term="meta engineering" /><category term="python" /><category term="machine learning" /><category term="lazy import" /><summary type="html"><![CDATA[머신러닝 세계에서 시간은 곧 생명입니다. 머신러닝 모델이 초기 학습 데이터를 처리하는 과정에서 소요되는 1밀리초의 차이는 생산성과 실험 과정에 아주 극명한 차이를 가져올 수 있습니다. 메타에서는 Lazy Imports 방식과 파이썬 런타임 Cinder를 도입하면서]]></summary></entry><entry><title type="html">Django의 DB 커넥션 관리</title><link href="https://miintto.github.io/docs/django-db-connection" rel="alternate" type="text/html" title="Django의 DB 커넥션 관리" /><published>2024-08-03T00:00:00+00:00</published><updated>2024-08-03T00:00:00+00:00</updated><id>https://miintto.github.io/docs/django-db-connection</id><content type="html" xml:base="https://miintto.github.io/docs/django-db-connection"><![CDATA[<p>Django 혹은 Spring 같은 웹프레임워크를 사용하여 어플리케이션을 구축하는 경우 대부분 데이터베이스 연동이 필요한데, 각 프레임워크마다 효율적으로 데이터베이스 커넥션을 관리하기 위해 여러 기술을 사용하고 있습니다.</p>

<p>가장 많이 사용되는 기술은 커넥션 풀링 기법입니다.
여러 JDBC 기반 라이브러리나 SQLAlchemy 에서는 해당 방식으로 데이터베이스 연결을 관리하고 있습니다.
반면 Django는 커넥션 풀 기능을 지원하지 않으며 대신 Persistent Connections 방식을 채택하여 커넥션 수명 주기를 관리합니다.
아래에서 Django를 중심으로 커넥션을 관리하는 방법을 정리해 보았습니다.</p>

<hr />

<h1 id="1-connection-pooling">1. Connection Pooling</h1>

<p><strong>커넥션 풀링</strong>(Connection Pooling)이란 데이터베이스와 연결된 객체를 미리 일정 개수만큼 생성하여 풀(Pool)에 저장해 두었다가, 데이터베이스와 상호작용이 필요할 때마다 풀에서 가져와 사용하는 기법입니다.
이러한 방식을 사용하면 데이터베이스 통신 과정에서의 발생하는 오버헤드를 줄일 수 있습니다.</p>

<p><img src="/img/posts/django-db-connection-3-way-handshake.png" style="max-width:480px" /></p>

<p>일반적으로 어플리케이션은 데이터베이스와 통신할 때 TCP/IP 프로토콜을 사용합니다.
TCP 통신은 3-way Handshaking 절차를 거치며 서버와 클라이언트가 서로를 검증한 후에 실시간 통신을 진행하게 되는데,
만약 어플리케이션으로 요청이 발생할 때마다 데이터베이스와 TCP 통신을 시작하게 된다면 불필요하게 통신 시간이 길어질 수 있습니다.
이때 데이터베이스와 미리 통신이 이루어진 커넥션을 계속 재사용한다면 매번 통신을 시작하는 오버헤드를 줄이고 응답 시간을 단축할 수 있습니다.</p>

<p>풀에 담겨있는 커넥션 객체들은 데이터베이스와 3-way Handshaking 과정이 완료되어 곧바로 가져와 데이터베이스와 통신이 가능한 상태입니다.
새로운 요청이 들어와 데이터베이스 작업이 필요한 경우 커넥션 풀에서 유휴 커녁션(Idle Connection)을 할당받아 사용합니다.
사용이 끝나면 연결을 종료하지 않고 다시 커넥션 풀에 반환하여 재사용할 수 있게 합니다.</p>

<p>여러 라이브러리에서 커넥션 풀 기능을 지원하고 있습니다.
Spring에서는 HikariCP를 사용하여 어플리케이션 실행 시 <code class="language-plaintext highlighter-rouge">minimumIdle</code> 개수만큼 커넥션을 만들고 <code class="language-plaintext highlighter-rouge">maximumPoolSize</code> 값으로 커넥션 개수를 조절할 수 있습니다.
SQLAlchemy에서도 엔진 생성 시에 커넥션 개수를 조정하기 위해 <code class="language-plaintext highlighter-rouge">pool_size</code>, <code class="language-plaintext highlighter-rouge">max_overflow</code> 인자를 사용할 수 있습니다.</p>

<hr />

<h1 id="2-persistent-connections">2. Persistent Connections</h1>

<p>Django 에서는 커넥션 풀링 대신 <strong>Persistent Connections</strong>(지속 연결) 방식으로 커넥션을 관리하는데, 한 번 연결한 커넥션을 곧바로 종료하지 않고 일정 시간이 지난 후에 종료되도록 하여 이어지는 요청에서도 커넥션을 재사용하도록 합니다.</p>

<h2 id="21-settings">2.1 Settings</h2>

<p>커넥션의 수명 주기는 세팅 파일에서 <code class="language-plaintext highlighter-rouge">CONN_MAX_AGE</code> 옵션으로 조정할 수 있습니다.
Integer 타입으로 초 단위로 설정할 수 있으며 따로 정의하지 않는 경우에는 기본적으로 0으로 설정됩니다.</p>

<p>아래와 같이 데이터베이스마다 커넥션 수명 주기를 설정할 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">DATABASES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">default</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">ENGINE</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">django.db.backends.postgresql</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">NAME</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">database</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">USER</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">PASSWORD</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">password</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">HOST</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">localhost</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">PORT</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">5432</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">CONN_MAX_AGE</span><span class="sh">"</span><span class="p">:</span> <span class="mi">60</span><span class="p">,</span>  <span class="c1"># 최대 수명 시간 (초 단위)
</span>    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>기본값이 0이므로 별다른 설정을 하지 않는다면 매 요청마다 커넥션을 연결하고 종료하게 됩니다.
<code class="language-plaintext highlighter-rouge">CONN_MAX_AGE</code> 값을 <code class="language-plaintext highlighter-rouge">None</code>으로 설정하는 경우에는 해당 커넥션을 끊지 않고 계속 연결합니다.</p>

<h2 id="22-lifecycle">2.2 Lifecycle</h2>

<p>Django는 기본적으로 스레드(Thread)마다 데이터베이스 커넥션을 관리합니다.
웹 서버가 여러 스레드를 사용하여 요청을 처리하는 경우 스레드마다 데이터베이스 커넥션이 생성되며, 스레드 간에는 커넥션이 공유되지 않습니다.</p>

<p><img src="/img/posts/django-db-connection-lifecycle.png" style="max-width:480px" /></p>

<p>커넥션은 최초 쿼리를 실행할 때 생성되며, <code class="language-plaintext highlighter-rouge">CONN_MAX_AGE</code> 값을 기반으로 해당 커넥션의 수명이 설정됩니다.
이후 데이터베이스 통신이 필요할 때도 해당 커넥션을 계속 사용합니다.</p>

<p>수명이 다한 커넥션을 종료하는 작업은 signal에 등록되어 있어서 미들웨어 스택에 진입하기 전에 한 번, 종료될 때 한 번씩 실행됩니다.
해당 시점에 커넥션의 수명이 끝난 경우나 데이터베이스 처리 중 에러가 발생하여 다시 재사용할 수 없다고 판단되는 경우 커넥션을 종료합니다.
이러한 방식으로 특정 커넥션에 문제가 발생하더라도 다음 요청에서는 새로운 커넥션을 사용하게 되어 에러의 영향을 받지 않게 됩니다.</p>

<p>어떻게 보면 커넥션 풀링 기법과 많은 점에서 유사합니다.
생성된 커넥션을 각 스레드마다 관리하는 것과 풀에서 한 번에 관리하는 방식의 차이일 뿐이지 한 번 연결된 객체를 종료하지 않고 재사용하여 매번 커넥션을 여닫는 비용을 줄인다는 원리는 동일합니다.
오히려 커넥션을 각 스레드에서 관리하기 때문에 커넥션을 할당받고 반환하는 작업이 생략되어 커넥션 풀링보다 더 효율적인 경우도 있습니다.</p>

<p>만약 많은 트래픽이 유입되어 데이터베이스 연결이 빈번해지는 경우 <code class="language-plaintext highlighter-rouge">CONN_MAX_AGE</code> 값을 잘 조절하면 매 요청마다 커넥션을 생성하는 비효율을 줄일 수 있습니다.
하지만 커넥션 수명 주기를 늘릴수록 많은 연결을 유지해야 하는 데이터베이스의 부하가 커지게 됩니다.
따라서 데이터베이스 사양과 요청 트래픽 및 여러 조건을 고려하여 적절한 수치를 찾아야 합니다.</p>

<h2 id="23-source">2.3 Source</h2>

<p>Django 내부에서는 아래와 같이 구현되어 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># django/db/backends/base/base.py
</span><span class="k">class</span> <span class="nc">BaseDatabaseWrapper</span><span class="p">:</span>
    <span class="bp">...</span>

    <span class="nd">@async_unsafe</span>
    <span class="k">def</span> <span class="nf">connect</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">데이터베이스 연결
        </span><span class="sh">"""</span>
        <span class="bp">...</span>

        <span class="n">self</span><span class="p">.</span><span class="n">health_check_enabled</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">settings_dict</span><span class="p">[</span><span class="sh">"</span><span class="s">CONN_HEALTH_CHECKS</span><span class="sh">"</span><span class="p">]</span>
        <span class="n">max_age</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">settings_dict</span><span class="p">[</span><span class="sh">"</span><span class="s">CONN_MAX_AGE</span><span class="sh">"</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">close_at</span> <span class="o">=</span> <span class="bp">None</span> <span class="k">if</span> <span class="n">max_age</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">time</span><span class="p">.</span><span class="nf">monotonic</span><span class="p">()</span> <span class="o">+</span> <span class="n">max_age</span>

        <span class="bp">...</span>

    <span class="k">def</span> <span class="nf">close_if_unusable_or_obsolete</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">에러가 발생하거나 수명이 다한 경우 현재 커넥션 종료 처리
        </span><span class="sh">"""</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">connection</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">health_check_done</span> <span class="o">=</span> <span class="bp">False</span>
            <span class="c1"># 어플리케이션의 autocommit 설정이 복원되지 않았다면 곧바로 커넥션 종료
</span>            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_autocommit</span><span class="p">()</span> <span class="o">!=</span> <span class="n">self</span><span class="p">.</span><span class="n">settings_dict</span><span class="p">[</span><span class="sh">"</span><span class="s">AUTOCOMMIT</span><span class="sh">"</span><span class="p">]:</span>
                <span class="n">self</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
                <span class="k">return</span>

            <span class="c1"># 지난 커밋이나 롤백 과정에서 에러가 발생한 경우 커넥션이 동작하는지 검증
</span>            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">errors_occurred</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="nf">is_usable</span><span class="p">():</span>
                    <span class="n">self</span><span class="p">.</span><span class="n">errors_occurred</span> <span class="o">=</span> <span class="bp">False</span>
                    <span class="n">self</span><span class="p">.</span><span class="n">health_check_done</span> <span class="o">=</span> <span class="bp">True</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">self</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
                    <span class="k">return</span>

            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">close_at</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">time</span><span class="p">.</span><span class="nf">monotonic</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">close_at</span><span class="p">:</span>
                <span class="n">self</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
                <span class="k">return</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">BaseDatabaseWrapper</code> 클래스는 데이터베이스의 연결을 관리하는 구현체입니다.
트랜잭션 커밋, 롤백 등과 같은 기본적인 기능을 제공하며, Oracle이나 MySQL, PostgreSQL 등의 데이터베이스마다 해당 클래스를 상속받아 구체적인 구현을 제공합니다.</p>

<p><code class="language-plaintext highlighter-rouge">CONN_HEALTH_CHECKS</code> 값을 <code class="language-plaintext highlighter-rouge">True</code>로 설정하면 커넥션을 재사용할 때마다 <code class="language-plaintext highlighter-rouge">SELECT 1</code> 실행(PostgreSQL) 혹은 핑(Ping) 테스트(Oracle, MySQL)를 진행하면서 커넥션이 유효하지 않다고 판단되는 경우 종료 처리합니다.</p>

<p>데이터베이스와 최초 연결하는 시점에 <code class="language-plaintext highlighter-rouge">connect()</code> 함수가 실행되어 <code class="language-plaintext highlighter-rouge">CONN_MAX_AGE</code> 값을 기반으로 종료 시점이 계산됩니다.
그리고 <code class="language-plaintext highlighter-rouge">close_if_unusable_or_obsolete()</code> 함수는 커넥션 종료 처리를 담당합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># django/db/__init__.py
</span><span class="k">def</span> <span class="nf">close_old_connections</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">conn</span> <span class="ow">in</span> <span class="n">connections</span><span class="p">.</span><span class="nf">all</span><span class="p">(</span><span class="n">initialized_only</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="n">conn</span><span class="p">.</span><span class="nf">close_if_unusable_or_obsolete</span><span class="p">()</span>

<span class="n">signals</span><span class="p">.</span><span class="n">request_started</span><span class="p">.</span><span class="nf">connect</span><span class="p">(</span><span class="n">close_old_connections</span><span class="p">)</span>
<span class="n">signals</span><span class="p">.</span><span class="n">request_finished</span><span class="p">.</span><span class="nf">connect</span><span class="p">(</span><span class="n">close_old_connections</span><span class="p">)</span>
</code></pre></div></div>

<p>종료 처리 함수는 <code class="language-plaintext highlighter-rouge">signals.request_started</code>에 등록되어 요청이 어플리케이션에 진입하여 미들웨어 스택 실행 전에 처리됩니다.
또한 <code class="language-plaintext highlighter-rouge">signals.request_finished</code>에도 등록되어 있어서 요청이 끝나기 전에 한 번 더 처리됩니다.</p>

<hr />

<p>References</p>

<ul>
  <li><a href="https://docs.djangoproject.com/en/5.0/ref/databases/#persistent-connections">Databases | Django documentation | Django</a></li>
  <li><a href="https://seungho-jeong.github.io/technology/computer-science/django-db-connections/">Django에서 DB Connection 관리   오늘보다 내일 더 잘하는 개발자</a></li>
  <li><a href="https://spoqa.github.io/2018/01/17/connection-pool-of-sqlalchemy.html">Spoqa 기술 블로그 | SQLAlchemy의 연결 풀링 이해하기</a></li>
  <li><a href="https://groups.google.com/g/django-developers/c/NwY9CHM4xpU">Database pooling vs. persistent connections</a></li>
</ul>]]></content><author><name>miintto</name></author><category term="django" /><category term="django" /><category term="connection pool" /><category term="persistent connections" /><summary type="html"><![CDATA[Django 혹은 Spring 같은 웹프레임워크를 사용하여 어플리케이션을 구축하는 경우 대부분 데이터베이스 연동이 필요한데, 각 프레임워크마다 효율적으로 데이터베이스 커넥션을 관리하기 위해 여러 기술을 사용하고 있습니다.]]></summary></entry><entry><title type="html">Django와 FastAPI 간의 성능 테스트</title><link href="https://miintto.github.io/docs/django-fastapi-performance-testing" rel="alternate" type="text/html" title="Django와 FastAPI 간의 성능 테스트" /><published>2024-07-28T00:00:00+00:00</published><updated>2024-07-28T00:00:00+00:00</updated><id>https://miintto.github.io/docs/django-fastapi-performance-testing</id><content type="html" xml:base="https://miintto.github.io/docs/django-fastapi-performance-testing"><![CDATA[<p>2022년도 즈음부터 FastAPI가 부상하기 시작했는데, 기존 Django 프로젝트만 운영해 본 필자는 동일한 로직을 FastAPI로 옮긴다면 성능이 얼마나 달라질까 하는 생각을 많이 해보았습니다.
현재에도 Django 와 FastAPI 각각 프로덕션 레벨에서 운영해 보았지만 서로 성격이 달라서 직접적으로 비교하기는 애매했습니다.</p>

<p>그래서 아예 이번 기회에 서버와 데이터베이스 환경을 동일하게 맞추어주고 같은 로직을 프레임워크만 바꾸어 테스트한다면 어떤 차이가 있을지 확인해 보았습니다.</p>

<hr />

<h1 id="1-scenario">1. Scenario</h1>

<p>먼저 일반적인 커머스 플랫폼을 가정하여 사용자가 상품을 조회한 후 주문하는 시나리오를 세웠습니다.
마치 이벤트 오픈 시점에 많은 사용자가 몰려들어 주문이 발생하는 상황처럼 과도한 트래픽이 유입되는 환경에서도 서버가 정상적으로 동작하는지 여부를 모니터링하고자 합니다.</p>

<p><img src="/img/posts/django-fastapi-performance-testing-flow.png" style="max-width:720px" /></p>

<p>관련된 사용자 행동을 정의하여 <em>상품 리스트 조회 → 상품 조회 → 주문 요청 → 주문 조회</em> 순으로 진행되도록 하였습니다.</p>

<p>각 사용자 행동마다 하나의 API를 할당하였고 API 최대 응답 시간을 100ms로 설정하여 기준치가 넘어가는 시점을 기록하였습니다.</p>

<hr />

<h1 id="2-testing-environment">2. Testing Environment</h1>

<h2 id="21-server">2.1 Server</h2>

<p>어플리케이션 서버는 개인적으로 사용 중이던 <strong>AWS Lightsail</strong> 인스턴스를 활용하였습니다.
서버 사양은 vCPU 1개에 메모리 512MB 정도 되었는데 서버 사양을 높게 잡아버리면 자칫하다가는 오히려 데이터베이스가 소화를 못 할 수 있을 것 같아서 데이터베이스 사양보다는 작게 잡았습니다.</p>

<p>데이터베이스는 <strong>PostgreSQL</strong>을 사용하였습니다.
데이터베이스 특성상 <a href="/docs/postgres-vacuum">Vacuum</a>으로 인한 성능 차이가 발생할 수 있을 것 같아서 매 테스트마다 데이터베이스 및 테이블을 새로 생성해 주었습니다.</p>

<p><img src="/img/posts/django-fastapi-performance-testing-erd.png" style="max-width:420px" /></p>

<p>테이블 구조는 최대한 단순하게 설계하였습니다.
상품 품목에 상품 수량과 판매 수량을 관리하여 주문이 일어날 때마다 판매 수량을 더해주도록 하였습니다.</p>

<p>구체적인 스펙은 아래와 같습니다</p>

<ul>
  <li>서버: Amazon Linux 2 AMI (512MB RAM / 1 vCPU)</li>
  <li>데이터베이스: PostgreSQL 12.7 (1GB RAM/ 2 vCPU)</li>
  <li>웹서버: Nginx 1.27.0</li>
  <li>언어: Python 3.12</li>
</ul>

<p>기본적인 환경은 이렇게 통일하고 프레임워크만 <strong>Django 4.2</strong>와 <strong>FastAPI 0.111</strong>로 설정하였습니다.
테스트에 사용한 어플리케이션 소스코드는 깃헙에 올려두었으니 <a href="https://github.com/miintto/django-app">django-app</a>, <a href="https://github.com/miintto/fastapi-app">fastapi-app</a> repo를 참고 부탁드립니다.</p>

<h2 id="22-testing-tool">2.2 Testing Tool</h2>

<p>테스트 도구로는 파이썬 기반 테스트 도구 <strong>Locust</strong>를 활용하였습니다.
로컬 환경에서 Locust 서버를 실행하고 도메인이 설정된 어플리케이션 서버에 요청을 보내는 방식으로 진행했습니다.</p>

<p><img src="/img/posts/django-fastapi-performance-testing-locust-flow.png" style="max-width:480px" /></p>

<p>Locust를 사용하면 간단하게 스크립트를 작성하여 테스트 시나리오를 세울 수 있으며, 테스트 진행 시에는 요청 수치를 일정하게 늘려가며 점점 부하를 줄 수 있습니다.
Gevent 기반으로 실행되는데, user마다 각각 그린 스레드(green thread)에 할당되어 서버에 요청을 보내게 됩니다.</p>

<hr />

<h1 id="3-results">3. Results</h1>

<h2 id="31-fastapi-report">3.1 FastAPI Report</h2>

<p>먼저 FastAPI 어플리케이션 테스트를 진행하였습니다.</p>

<p><img src="/img/posts/django-fastapi-performance-testing-report-fastapi-1.png" style="max-width:540px" /></p>

<p>처음 user를 40으로 설정하여 5분 정도 유지하였고, 상황을 보아서 50, 60, 70으로 차차 높였습니다.
user 수치를 60으로 올린 후에 평균 응답 시간이 100ms를 넘어섰고, 70까지 올렸을 때는 160ms에 다다랐습니다.</p>

<p><img src="/img/posts/django-fastapi-performance-testing-report-fastapi-2.png" style="max-width:540px" /></p>

<p>마지막에 user를 80까지 높여보았으나 서버가 죽어버려서 응답을 받지 못하는 상황까지 이루어졌습니다.
RPS 수치를 보아 초당 최대 100개의 요청까지 처리 가능한 것으로 보여집니다.</p>

<p><img src="/img/posts/django-fastapi-performance-testing-cpu-usage-fastapi.png" style="max-width:420px" /></p>

<p>테스트가 진행되던 시점의 서버 CPU 수치를 확인해 보았는데 서버 자원을 잘 사용하는 것으로 보여집니다.</p>

<h2 id="32-persistent-connections-in-django">3.2 Persistent Connections in Django</h2>

<p>그다음으로 Django 환경으로 변경하여 테스트를 진행했는데 사소한 문제가 있었습니다.</p>

<p><img src="/img/posts/django-fastapi-performance-testing-report-django-1.png" style="max-width:540px" /></p>

<p>최초 테스트에서 user 수가 20을 넘어가면서부터 기준치 100ms를 훌쩍 넘어가 버렸습니다.
아무리 Django가 무겁다고 하지만 이렇게나 차이가 나는 부분이 의아하여 다시 로직을 점검해 보았습니다.</p>

<p>확인해 보니 테스트 시점에 데이터베이스의 부하가 다소 높게 나타났습니다.
이전 FastAPI 테스트에서는 데이터베이스 CPU 사용률이 20% 미만으로 유지되었지만 이번 테스트에서는 40%까지 치솟은 반면 어플리케이션 서버의 CPU 사용률은 50% 정도밖에 되지 않았습니다.
Django의 데이터베이스 연결 및 처리 쪽에 문제가 있는 것 같아 해당 부분을 다시 한번 조사해 보았습니다.</p>

<p>기본적으로 Django에서는 커넥션 풀(connection pool) 기능을 지원하지 않고 있습니다.
대신 <strong>지속 연결</strong>(persistent connections) 방식으로 한 번 연결한 커넥션의 유지 시간을 조절할 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># settings.py
</span><span class="n">DATABASES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">default</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">ENGINE</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">django.db.backends.postgresql</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">NAME</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">test-db</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">USER</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">PASSWORD</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">****</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">HOST</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">test.rds.amazonaws.com</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">PORT</span><span class="sh">"</span><span class="p">:</span> <span class="mi">5432</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">CONN_MAX_AGE</span><span class="sh">"</span><span class="p">:</span> <span class="mi">60</span><span class="p">,</span>  <span class="c1"># 60초로 설정
</span>    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>데이터베이스 세팅에서 <code class="language-plaintext highlighter-rouge">CONN_MAX_AGE</code> 값으로 커넥션의 최대 수명 기간을 설정할 수 있습니다.
기본값이 0이라 별다른 설정이 없다면 매 HTTP 요청마다 데이터베이스 커넥션을 연결하고 끊는 과정을 반복하게 됩니다.
만약 요청이 많아진다면 데이터베이스 연결 과정에서 불필요한 오버헤드가 발생할 수 있습니다.
Django의 Persistent Connections 관련해서는 <a href="/docs/django-db-connection">다음 포스트</a>에서 좀 더 자세히 설명하겠습니다.</p>

<p>해당 설정값을 60초로 변경해 두고 다시 테스트를 진행했습니다.</p>

<h2 id="33-django-report">3.3 Django Report</h2>

<p>두 번째 Django 테스트에서는 처음 user 수치를 30으로 낮추어 시작했습니다.</p>

<p><img src="/img/posts/django-fastapi-performance-testing-report-django-2.png" style="max-width:540px" /></p>

<p>User 수치를 40, 50, 60, 70으로 올리면서 상황을 지켜보았는데, FastAPI에서와 같이 user 60에서 평균 응답 시간이 100ms를 초과하였습니다.
RPS도 100 내외를 유지하였습니다.</p>

<p><img src="/img/posts/django-fastapi-performance-testing-cpu-usage-django.png" style="max-width:420px" /></p>

<p>어플리케이션 서버 CPU 사용률도 문제없어 보입니다.</p>

<hr />

<h1 id="4-conclusion">4. Conclusion</h1>

<p>결론적으로 두 프레임워크에서 비슷한 결과가 나왔습니다.
한쪽이 어느 정도 우월하게 나와야 흥미로웠을 텐데 어쩌다 보니 좀 싱겁게 되어버렸습니다.</p>

<p>개인적으로 ORM 관련해서는 Django ORM 보다 SQLAlchemy의 성숙도가 뛰어나다고 생각해서 데이터베이스 CRUD 작업 관련해서는 FastAPI가 효율이 더 좋을 거라고 예상했었는데 막상 뚜껑을 열어보니 그다지 큰 차이가 없었습니다.
사실 ORM이 직접적으로 어떤 기능을 한다기보다는 파이썬 코드를 SQL로 변환하는 게 주요 역할이라서 동일한 데이터베이스에서는 비슷한 결과가 나오는 게 당연한 수순인 것 같습니다.</p>

<p>이번 테스트의 결과는 Django와 FastAPI의 전반적인 결과가 아닌 단순 데이터베이스 CRUD 상황에만 국한됩니다.
이미지 처리나 HTML 렌더링 같은 다른 로직이나 개발 속도까지도 고려한다면 얼마든지 다른 결과가 나타날 수 있습니다.</p>

<hr />

<p>References</p>

<ul>
  <li><a href="https://docs.djangoproject.com/en/5.0/ref/databases/#persistent-connections">Databases | Django documentation | Django</a></li>
  <li><a href="https://chrisjune-13837.medium.com/postgres%EC%99%80-django%EC%9D%98-connection-%EA%B4%80%EB%A6%AC-5acf3f5c28a7">[DB] Postgres와 Django의 Connection 관리 | by chrisjune | Medium</a></li>
  <li><a href="https://americanopeople.tistory.com/260">(Django) DB Connection을 관리하는 방법</a></li>
</ul>]]></content><author><name>miintto</name></author><category term="django" /><category term="django" /><category term="fastapi" /><category term="locust" /><category term="persistent connections" /><summary type="html"><![CDATA[2022년도 즈음부터 FastAPI가 부상하기 시작했는데, 기존 Django 프로젝트만 운영해 본 필자는 동일한 로직을 FastAPI로 옮긴다면 성능이 얼마나 달라질까 하는 생각을 많이 해보았습니다. 현재에도 Django 와 FastAPI 각각 프로덕션 레벨에서 운영해 보았지만 서로 성격이 달라서 직접적으로 비교하기는 애매했습니다.]]></summary></entry><entry><title type="html">[번역] Cinder의 Lazy Import</title><link href="https://miintto.github.io/docs/meta-cinder-lazy-import" rel="alternate" type="text/html" title="[번역] Cinder의 Lazy Import" /><published>2024-07-08T00:00:00+00:00</published><updated>2024-07-08T00:00:00+00:00</updated><id>https://miintto.github.io/docs/meta-cinder-lazy-import</id><content type="html" xml:base="https://miintto.github.io/docs/meta-cinder-lazy-import"><![CDATA[<blockquote>
  <p>해당 포스트는 Meta 블로그의 <a href="https://developers.facebook.com/blog/post/2022/06/15/python-lazy-imports-with-cinder/">Python Lazy Imports With Cinder</a> 포스트를 번역한 글입니다.</p>

  <p>게시일: 2022.06.15</p>
</blockquote>

<h1 id="cinder의-lazy-import">Cinder의 Lazy Import</h1>

<p>파이썬은 <a href="https://xkcd.com/303/">별도 컴파일 과정 없이</a> 빠른 코드 수정 및 실행이 가능하다는 장점 덕분에 개발자 친화적인 언어로 널리 사용되고 있습니다.
하지만 대규모로 운영되는 인스타그램 서버를 로컬 개발 환경에 구성하는 경우 이러한 사용성 이점을 제대로 누리지 못하고 있었습니다.
파이썬 코드를 변경한 후 로컬 환경에서 서버를 재시작하면 이 과정에서 서버 시작 시간이 지연되어 평균적으로 <strong>50초</strong>가량 소요된다는 점이 주요 골칫거리였습니다.</p>

<p>메타에서는 Lazy Imports 기능을 도입하여 이러한 문제를 해결하였습니다.
Lazy Imports는 lazy loading에 명확하고도 강력한 메커니즘을 접목시킨 파이썬 런타임 기능인데,
이러한 방식으로 서버 재시작 시간을 <strong>70%</strong> 절감하여 매일마다 개발자들의 수백 시간을 절약하고 있습니다.</p>

<h2 id="식어버린-커피">식어버린 커피</h2>

<p>아침이 밝았습니다.
당신은 눈을 뜨자마자 따뜻한 커피 한 잔을 들고 컴퓨터 앞으로 향했습니다.
당신의 머리속에는 오늘 업무에 대한 무궁무진한 아이디어가 넘쳐납니다.
커피 한 모금을 마신 후 코드를 리베이스하고 로컬 서버를 실행합니다.
하루 일과가 시작되었습니다!</p>

<p>평소처럼 몇몇 파일을 건드렸고 서버를 다시 시작했습니다.
서버가 완전히 시작되기까지 어느 정도 시간이 소요됩니다.
서버가 잘 돌아가는 듯 싶더니.. 잠시 후 에러가 발생하였습니다.
그중 일부는 어디서 발생했는지조차 가늠이 되지 않아서 traceback을 확인하고자 몇몇 파일에 로깅을 추가했습니다.
다시 10초, 20초.., 60초가 지나갔지만, 여전히 서버는 재시작 중입니다.
그리고 또다시 에러가 발생했는데 추가했던 로깅 라인에 구문 에러가 포함되어 있었습니다.
에러를 수정한 후 서버를 재시작했고 또다시 하염없는 기다림이 시작되었습니다.
2분여가 지나고 나서야 에러 로그를 확인할 수 있었으며, 한 시간 후에는 버그를 고칠 수 있었습니다.
원인은 바로 이틀 전 제거한 <code class="language-plaintext highlighter-rouge">import</code> 구문이었는데 아침에 최신 코드를 리베이스하면서 불러오는 모듈이 꼬여버린 것이었습니다.</p>

<p>이 과정에서 소중한 아침 두 시간을 낭비했고 오늘 해야 할 일을 전혀 진행하지 못했습니다.
더군다나 아까 가져온 따끈한 커피는 이미 차게 식어버렸습니다.</p>

<p>이렇게 서버를 기다리는 시간이 하루 종일 쌓이게 되고 마찬가지로 다른 동료들도 동일한 문제를 겪게 됩니다.
그렇게 누적된 시간은 몇 시간, 며칠이 되어 결국 어마어마한 시간 낭비가 발생하고 있습니다.</p>

<h2 id="느려터진-서버-재시작">느려터진 서버 재시작</h2>

<p>인스타그램 서버를 구동할 때마다 <i>모듈을 불러오는 데 어마어마한 시간</i>이 소요됩니다.
어떤 경우에는 모듈끼리 밀접하게 연계되어 있어서 하나의 모듈을 불러오더라도 엮인 모듈들을 연쇄적으로 가져오게 되기도 합니다.</p>

<p>서버 재시작 시간은 2021년도 말에 약 <strong>25초</strong> 정도 소요되었습니다.
이 소요 시간은 수년간 골칫거리로 과거부터 지속적으로 늘어나고 있었습니다.
개발자들이 꾸준하게 최적화 해주지 않으면 시간은 급격하게 치솟았고 결국 2021년 신기록을 달성해 버렸습니다.
2021년 끝물에 최고점을 찍었을 때는 <strong>1분 30초</strong>까지 소요된 적이 있었습니다. 
이렇게 느려터진 서버를 기다리는 동안 엔지니어의 집중력이 흐려지고 하려던 일을 까먹을 모습이 불 보듯 뻔했습니다.</p>

<p><em>그렇다면 왜 이렇게 서버가 느려진 걸까요?</em></p>

<h2 id="코드베이스-복잡성">코드베이스 복잡성</h2>

<p>이렇게까지 서버 시작 시간이 느려진 주요한 요인은 바로 인스타그램의 <strong>코드베이스 복잡성 증가</strong>와 코드가 참조하고 있는 수많은 모듈 때문입니다.</p>

<p>Joshua Lear는 하루 온 종일을 쏟아부어서 이전까지 아무도 보지 못했던 인스타그램 서버 코드의 복잡한 의존성 그래프를 만들었습니다.
의존성 그래프를 시각화하는 스크립트를 약 <strong>3시간</strong> 정도 실행한 후 “거대한 검은 점” 하나가 출력되었습니다.
처음 그래프를 확인했을 때는 버그가 발생한 줄 알았지만 이내 인스타그램의 의존성이 이렇게 거대하다는 사실을 깨달았습니다.</p>

<p><img src="/img/posts/meta-cinder-lazy-import-img001.png" style="max-width:600px" />
<span class="caption text-muted">Joshua Lear의 인스타그램 의존성 그래프 (예술적 재창조)</span></p>

<p>실제로 인스타그램의 코드베이스 의존성은 모든 부분이 강하게 연결되어 있어서 마치 거대한 그물망과 같이 되어있습니다.
그래서 서버를 시작하는 것만으로도 약 <strong>28,000개</strong>의 모듈을 불러오게 되는데, 이 부분에서 모듈을 가져와 파이썬 함수 및 클래스 객체를 생성하는 데 상당한 시간이 소요됩니다.
좀 더 봐줄 만한 의존성 그래프는 Benjamin Woodruff가 현재 상태를 반영하여 다시 만들었고 그 결과물은 아래와 같습니다.</p>

<p><img src="/img/posts/meta-cinder-lazy-import-img002.png" style="max-width:600px" />
<span class="caption text-muted">현 시점 인스타그램 의존성 그래프, January 2022</span></p>

<p>그러면 과연 무엇이 문제였을까요?
코드에서 의존성이 심한 부분을 찾아서 제거해 주기만 하면 되는 걸가요?
이게 그렇게 간단하지만은 않습니다.</p>

<h2 id="순환-참조">순환 참조</h2>

<p>복잡한 코드와 강하게 얽힌 의존성은 재앙과도 같습니다.
의존성을 덜어내기 위해서 코드 리팩토링이 최선의 방법으로 여겨지지만 가장 큰 걸림돌은 바로 <strong>순환 참조</strong>입니다.
리팩토링을 시도하는 순간 사방에서 순환 참조가 발생할 수 있습니다.</p>

<p>순환 참조는 리팩토링을 까다롭게 만들고 이전에도 여러 장애를 일으켰습니다.
심지어 import 순서를 바꾸는 것조차도 순환 참조를 유발할 수 있습니다.</p>

<h2 id="한-줄기-빛">한 줄기 빛</h2>

<p>우리는 이전에 순환 참조를 걷어내고 의존성을 간결하게 하기 위해 모듈 리팩토링을 진행한 적이 있었습니다.
당시 Django Url, Notifications, Observers, 정규식 등과 같은 무거운 하위 시스템을 지연시키면서 신중하게 솔루션을 조정하였습니다.
이러한 작업은 어느 정도 효과가 있었지만 대신 많이 불안정했습니다.
수년 동안 수많은 시간을 쏟아부어 일일이 프로파일링하고 리팩토링하며 정리 작업을 진행했음에도 불구하고 코드 복잡성이 지속적으로 증가하면서 공들인 노력이 다시 수포로 돌아갔습니다.
이러한 리팩토링 과정은 너무 힘겨웠고, 불안정했으며, 확장성이 좋지도 않았습니다.</p>

<p>우리가 원했던 건 모든 걸 느긋하게 만들 강력한 방법이었습니다.</p>

<h2 id="lazy-imports">Lazy Imports</h2>

<p><img src="/img/posts/meta-cinder-lazy-import-img003.jpg" style="max-width:600px" />
<span class="caption text-muted">Creative Commons의 Geoff Gallice로부터 제공받은 두발가락나무늘보</span></p>

<p>우리는 좀 더 깔끔하고 손이 많이 가지 않으면서도 신뢰성 높고 영구적인 방법으로 모듈을 가져오는 시점을 늦출 방법이 필요했습니다.
더 이상 메소드 내부에서 import 호출을 하거나 <code class="language-plaintext highlighter-rouge">import_module()</code>, <code class="language-plaintext highlighter-rouge">__import__()</code> 를 사용하는 방식은 피하고 싶었습니다.
구상하고 있는 프로젝트는 야심 차고 위험했지만 저는 소매를 걷어붙이고 CPython를 파고들어 <a href="https://github.com/facebookincubator/cinder">Cinder</a> 내부에 <i>Lazy Imports</i>를 구현하기 시작했습니다.</p>

<p>Lazy Imports는 파이썬이 모듈을 불러오는 메커니즘을 바꾸어 모듈이 사용되는 시점에만 가져오도록 하였습니다.
구체적으로 매 <em>import</em> 호출 라인마다 즉시 모듈을 로드하지 않고 대신 <i>“지연 객체” 이름</i>을 생성합니다.
그 <i>이름</i>은 사용되기 전까지 지연 객체의 인스턴스로 남아있게 됩니다.
그리고 곧바로 다음 라인에서 사용되거나 혹은 몇 시간 후에 콜 스택에서 사용될 수도 있습니다.</p>

<p>몇 주 동안의 작업을 거친 끝에 프로토타입을 만들 수 있었습니다.
결과는 성공적이었습니다.
원하는 대로 잘 작동했으며 몹시 기대되었습니다.
하지만 앞으로 펼쳐질 험난한 싸움은 미처 생각하지 못했습니다.
힘들었던 부분은 이 구현체를 좀 더 견고하게 만들고 효율적으로 구성하며 안정적으로 배포되도록 하는 것이었습니다.
무엇보다도 이 기능을 위해 파이썬 핵심 부분을 변경하는건 제가 처음 예상한 것보다 훨씬 더 복잡한 과정이었습니다.
그리고 작업 도중에서야 발견하고 손대야 했던 수많은 난관이 있었습니다.</p>

<p>파이썬의 내부 동작 방식에 많은 변덕과 미묘한 차이가 존재하며, Lazy Imports에서 지연 객체 개념은 C언어에서 파이썬으로 넘어왔습니다.
Carl Meyer, Dino Viehland와 충분한 논의를 가진 끝에 저는 대부분의 기능을 파이썬 핵심부, 즉 파이썬 dictionary 내부로 옮기는 방식으로 다시 설계하기로 결정했습니다.
저는 무척이나 신났지만, 고도로 최적화된 dictionary 구현체를 자칫 잘못 건드렸다가는 심각한 성능 하락으로 이어질 수 있기 때문에 이 최적화 작업에 굉장히 심혈을 기울였습니다.</p>

<p>마침내 믿을만하고 효율적인 버전을 만들었습니다.
저는 수만 개의 인스타그램 서버 모듈에 Lazy Imports를 작동시켰고 성능 테스트를 진행하여 과연 운영 환경에서 성능 차이가 발생하지 않을지 확인하였습니다.
아니나 다를까 성능적으로 큰 변화가 없었고 운영 환경에서 어떠한 부정적인 양상도 발견되지 않았습니다.</p>

<h2 id="결과">결과</h2>

<p>2022년 1월에 해당 기능을 수천 개의 개발 &amp; 운영 환경에 문제없이 배포했고 곧바로 그래프에서 인스타그램 서버의 재시작 시간 차이를 확인할 수 있었습니다.</p>

<p><img src="/img/posts/meta-cinder-lazy-import-img004.png" style="max-width:480px" /></p>

<p><strong>모듈을 12배 적게</strong> 로드하면서 개발 서버의 평균 재시작 시간의 <strong>중앙값은 70% 줄어들었고 90th percentile 값은 60% 감소하였습니다.</strong>
동시에 우리가 매일 신경 쓰던 순환 참조 에러도 사실상 모두 제거되었습니다.
인스타그램 외의 다른 서버에서도 동일하게 <strong>50~70%</strong> 성능이 향상되었고 메모리 사용량은 <strong>20~40%</strong> 까지 감소했습니다.</p>

<p><img src="/img/posts/meta-cinder-lazy-import-img005.png" style="max-width:480px" />
<span class="caption text-muted">그래프의 어느 지점에서 Lazy Imports가 반영되었는지 보이시나요?</span></p>

<p>추가적인 결과는 <a href="https://github.com/facebookincubator/cinder/blob/cinder/3.8/CinderDoc/lazy_imports.rst#results">여기</a>를 참고해 주세요.</p>

<h2 id="시행착오">시행착오</h2>

<p>프로젝트를 진행하면서 여기 포스트에는 담지 못할 수많은 난관이 있었습니다.
중간중간 매우 까다로운 상황이 있었지만 그래도 대다수의 경우는 어렵지 않았습니다.
돌이켜보면 CPython의 몇 가지 버그 (<code class="language-plaintext highlighter-rouge">TypeDict</code> 관련 <a href="https://bugs.python.org/issue41249">bpo-41249</a>), 걷어내야만 했던 일부 라이브러리, 다듬어야 할 수많은 테스트 코드 정도가 기억에 남습니다.</p>

<p>제가 코드베이스에 Lazy Imports를 접목하면서 경험한바 Lazy Imports 적용 시 일반적으로 나타날 수 있는 문제들은 아래와 같습니다.</p>

<ol>
  <li>모듈을 불러올 때 발생하는 사이드 이펙트에 의존하는 문제:
    <ul>
      <li>모듈 import 시점에 임의로 어떤 로직이 실행되는 경우.</li>
      <li>상위 모듈에서 하위 모듈을 인자로 선언하여 의존하는 경우.</li>
    </ul>
  </li>
  <li>파이썬 동적 경로 관련 문제. 예를 들면 모듈 경로를 <code class="language-plaintext highlighter-rouge">sys.path</code>에 추가 후 import 후에 다시 제거하는 등의 경우.</li>
  <li><code class="language-plaintext highlighter-rouge">ModuleNotFoundError</code>를 포함한 모든 에러가 import 시점에서 모듈이 사용되는 시점으로 지연되면서 디버깅이 번거로워지는 문제.</li>
  <li>타입 annotation을 적용하는 경우 세심한 주의가 필요하며, 누락된 경우 Lazy Import가 깨져버리는 문제:
    <ul>
      <li>모듈마다 <code class="language-plaintext highlighter-rouge">from __future__ import annotations</code> 구문을 반드시 선언해야 하고,</li>
      <li>스트링 타입 어노테이션으로는 <code class="language-plaintext highlighter-rouge">typing.TypeVar()</code>와 <code class="language-plaintext highlighter-rouge">typing.NewType()</code>을 사용해야 하며,</li>
      <li>타입 alias는 <code class="language-plaintext highlighter-rouge">TYPE_CHECKING</code> 블럭 내부에서 처리해야합니다.</li>
    </ul>
  </li>
</ol>

<p>더 포괄적인 문제들은 <a href="https://github.com/facebookincubator/cinder/blob/cinder/3.8/CinderDoc/lazy_imports.rst#issues-and-gotchas">여기</a>를 참고해주세요.</p>

<hr />

<h2 id="강점">강점</h2>

<p>모듈을 사용 시점에 로드하는 lazy import가 완전히 새로운 개념이 아니고 간단한 방식이었지만, 아무도 CPython 내부에 직접 구현할 생각을 하지 못했으며 <a href="https://github.com/facebookincubator/cinder/blob/cinder/3.8/CinderDoc/lazy_imports.rst#prior-art">이전에 시도한 방식</a>도 지금 Cinder에 도입한 구현체와도 거리가 있습니다.
Lazy Imports의 강점은 아래와 같습니다.</p>

<ul>
  <li>파이썬을 지연시키는 목적으로 사용되는 패러다임 중 가장 <strong>자연스럽</strong>고, <strong>강력</strong>하며 <strong>명료</strong>한 방법입니다.</li>
  <li>적용하는데 크게 <strong>어렵지 않습니다</strong>. Lazy Imports 기능을 언어 레벨에 전역으로 설정해 두었으며 단일 모듈이나 표준 및 서드파티 라이브러리에도 적용 가능합니다.</li>
  <li><strong>효율적</strong>입니다. 저희 서버에 여러 테스트를 진행했는데, Lazy Imports를 추가하더라도 성능에 큰 영향을 미치지 않았습니다. 오픈소스 <a href="https://pyperformance.readthedocs.io/"><u>pyperformance</u></a>를 <strong>3회</strong> 실행하여 측정하였는데, Lazy Imports를 적용하지 않았을 때와 적용했을 때의 성능을 비교하여 아래와 같이 유의미한 결과를 얻었습니다.
<img src="/img/posts/meta-cinder-lazy-import-img006.png" style="max-width:360px" /></li>
  <li><strong>순환 참조가 발생하지 않습니다</strong>. 단순히 Lazy Imports가 직접 순환 참조를 걸러낸다는 의미가 아닙니다. 여전히 모듈 간의 순환 의존성이 존재할 수 있으나 대부분의 순환 의존성은 큰 문제가 없는 일반적인 상황입니다. 인스타그램에서는 매일마다 80개의 순환 참조 에러가 발견되었지만, 현재는 발생하지 않고 있습니다.</li>
</ul>

<h2 id="향후-전망">향후 전망</h2>

<ul>
  <li>Lazy Imports를 적절히 개선하여 메모리 확보, 서버 재시작 시간 개선뿐 아니라 인스타그램 서버의 성능까지 이끌어낼 수 있습니다.</li>
  <li>순환 참조에 대한 염려가 사라지면서 코드베이스의 품질을 높이는 새로운 길을 열어줍니다. 리팩토링이 더욱 간편해지고 불가능했던 일이 가능해집니다.</li>
  <li>외부 서드파티 라이브러리와 협력하여 Lazy Imports와 호환 가능하도록 하여 많은 어플리케이션이 활용할 수 있도록 합니다.</li>
  <li><a href="https://peps.python.org/pep-0690/"><u>보다 광범위한 세계관에서 Lazy Import를 사용할 수 있도록 파이썬 프로젝트에 기여합니다.</u></a></li>
</ul>

<hr />

<h1 id="python-lazy-imports-with-cinder">Python Lazy Imports With Cinder</h1>

<p>Python is widely touted as one of the most developer-friendly languages, thanks to the fast feedback loop that comes from <a href="https://xkcd.com/303/">not needing to compile</a>.
However, when used at scale in Instagram Server, we’ve found a major usability problem when developing locally; <em>every single change</em> to any Python file requires developers to perform a slow server reload (<strong>~50 seconds</strong> on average!) to observe the effects of their change.</p>

<p>At Meta, we’ve tackled this problem by creating Lazy Imports: a Python runtime feature that provides a transparent and robust mechanism to lazy loading.
Using this technique, we’ve saved hundreds of developer hours per day by reducing the cost of reloads by <strong>~70%</strong>, allowing developers to iterate more quickly.</p>

<h2 id="a-cold-cup-of-coffee">A Cold Cup of Coffee</h2>

<p>It all starts one morning.
You wake up, pour yourself a hot cup of coffee and head to your laptop to start a productive day.
You have a ton of great ideas about the things you are going to accomplish during the day.
You rebase and the server is reloading while you take a sip of coffee. The day begins!</p>

<p>As usual, you edit a few files, so the server needs to reload.
It takes some time to restart and we’re all good… until…
there’s this bug that gives you an error, one of those obscure things you know nothing about or where it comes from.
You need to add some logging so you modify one of the files listed in the traceback… ten seconds, twenty, sixty seconds… server still reloading… bang!
Syntax error in your logging line!
You fix the error and then save the file… server starts reloading again… reloading… reloading some more…
After two minutes, you are ready to see your logs.
An hour later, you finally nail the bug, it was <em>that one line</em> you removed two days ago, an <code class="language-plaintext highlighter-rouge">import</code>, which unfortunately triggered an obscure <em>import cycle</em> after fetching and rebasing the latest code.</p>

<p>To this point, you’ve just burned a couple hours of your morning and got distracted from what you were supposed to get done today.
Worst of all, <em>your coffee is now <strong>cold</strong>!</em></p>

<p>You get the picture; waiting times for server reloads pile up throughout your day, and <em>everyone else’s</em> for that matter.
That adds up quickly.
Soon minutes become hours and hours become days, <strong>all wasted time</strong>.</p>

<h2 id="slow-server-reloads">Slow Server Reloads</h2>

<p>Starting Instagram Server, we spend a <em>large amount of time</em> loading modules.
Often, modules are highly entwined, which makes it hard to stop an <strong>Import Domino Effect</strong> when importing anything.</p>

<p>A server reload took around <strong>25 seconds</strong> in late 2021.
This time has historically been <em>constantly</em> regressing — an ongoing battle for years.
If we don’t pay close attention to keeping it optimized, reload times go up quickly; through 2021 it saw new heights.
At its peak, by the end of the year, some reload times were taking as long as <strong>1.5 minutes</strong>.
This, unfortunately, is the perfect amount of time for engineers to get distracted by something shiny and forget what they are doing.</p>

<p><em>Why is the server so slow?</em></p>

<h2 id="codebase-complexity">Codebase Complexity</h2>

<p>The main reason for slow reloads is the <em><strong>increasingly complex codebase</strong></em> that we have in Instagram, together with the fact that we have a ton of modules making lots of references.</p>

<p>If you have never seen an image of how complex the dependency graph of Instagram Server code is, Joshua Lear spent a full day preparing one.
After <strong>3 hours</strong> of running a modified dependency visualization script, he came back to a <em>“large, black ball.”</em>
At first he thought the dependency analyzer had a bug, but it turns out Instagram Server’s dependency graph was a giant circle.</p>

<p><img src="/img/posts/meta-cinder-lazy-import-img001.png" style="max-width:600px" />
<span class="caption text-muted">Recreation (artistic interpretation) of Instagram Dependency Graph, by Joshua Lear</span></p>

<p>In all truth, the dependency graph in the Instagram codebase is a <em>big ugly mesh</em>; everything is very tightly connected.
Just starting the server automatically triggers loading a huge number of modules, about <strong>28,000</strong>, and most of that startup time is spent, literally, just importing modules, creating Python functions and class objects.
A nicer looking dependency graph was first provided by Benjamin Woodruff and updated to reflect the current state:</p>

<p><img src="/img/posts/meta-cinder-lazy-import-img002.png" style="max-width:600px" />
<span class="caption text-muted">Real Instagram Dependency Graph, January 2022</span></p>

<p>So what’s the problem?
Just figure out the heavy dependencies and remove them from the code in the hot path, right? Not quite.</p>

<h2 id="circular-imports">Circular Imports</h2>

<p>Highly complex code and entwined dependencies are a recipe for disaster.
Refactoring to keep dependencies clean and minimal sounds like the obvious fix, but the <em>biggest point of friction is <strong>circular imports</strong></em>. As soon as you start trying to refactor, import cycles pop up everywhere.</p>

<p>Import cycles make refactoring harder and have historically produced several outages; even changing the import order can trigger an import cycle somewhere, either immediately or pretty soon for someone else.</p>

<h2 id="a-beam-of-light">A Beam of Light</h2>

<p>In the past we’ve tried to refactor modules to break import cycles and simplify the dependency graph.
We’ve tried carefully tailoring solutions by making expensive subsystems lazy, e.g., <em>Django Urls, Notifications, Observers, even Regular Expressions</em>.
This works to a certain extent, but produces fragile solutions.
Through the years, countless hours were spent trying to solve this by manually profiling, refactoring and cleaning things up, only to realize that much goes down the drain pretty soon as code and complexity continues growing.
This process is hard, fragile and does not scale well.</p>

<p>What we needed was a robust way of lazyfing all things.</p>

<h2 id="lazy-imports-1">Lazy Imports</h2>

<p><img src="/img/posts/meta-cinder-lazy-import-img003.jpg" style="max-width:600px" />
<span class="caption text-muted">Two-toed sloth courtesy of Geoff Gallice via Creative Commons</span></p>

<p>We needed a more transparent, automatic, reliable and permanent way to make things lazy, instead of manually trying to make things <em>lazy</em> by using <em>inner imports</em>, <code class="language-plaintext highlighter-rouge">import_module()</code>, or <code class="language-plaintext highlighter-rouge">__import__()</code>.
The envisioned project was ambitious and risky, but I rolled my sleeves, dove deep into CPython and started implementing <em>Lazy Imports</em> in <a href="https://github.com/facebookincubator/cinder">Cinder</a>.</p>

<p>Lazy Imports changes the mechanics of how imports work in Python so that modules are imported only when they are used.
At its core, every single <em>import</em> (e.g., <code class="language-plaintext highlighter-rouge">import foo</code>) won’t immediately load and execute the module, it will instead create a <em>“deferred object” name</em>.
That <em>name</em> will internally remain an instance of a <em>deferred object</em> until the <em>name</em> is used, which could be in the next line after importing it, or in a deep call stack, many hours later.</p>

<p>After a few weeks working on it, I was able to get a prototype.
It was working, it was good and very promising; little did I know of the uphill battle that lay ahead.
The hard part was going to be making things rock solid, making the implementation super efficient and rolling it out without too many hiccups.
Changing the Python semantics, the way this feature does, would prove to be much more complex than I initially thought, and there were a lot of unexpected wrinkles to discover and fix along the way.</p>

<p>There are many quirks and nuances in the way Python works internally, and the Lazy Imports <em>deferred objects</em> unexpectedly leaked out of the C world into Python.
After some very productive discussions with Carl Meyer and Dino Viehland, I decided to redesign the machinery and move most of it deeper, into the heart of Python: the dictionary internals.
I was very excited, but modifying the highly optimized implementation of dictionaries could lead to a really bad performance penalty, so I took a lot of care on this part and optimizations took a fair amount of time.</p>

<p>At last, I was able to get a reliable and efficient version working.
I enabled Lazy Imports in tens of thousands of Instagram Server modules and started running performance experiments on it to see if it would make any performance difference in production (it shouldn’t).
Sure enough, the net looked like almost a wash, we didn’t see any clear signal that the implementation would affect negatively in production and I finally had a perf neutral build too.</p>

<h2 id="results">Results</h2>

<p>In early January 2022, we rolled out to thousands of production and development hosts with no major issues, and we could immediately see the difference in Instagram Server start times in the graphs:</p>

<p><img src="/img/posts/meta-cinder-lazy-import-img004.png" style="max-width:480px" /></p>

<p>By loading <strong>~12x less modules</strong>, we measured a ~<strong>70% reduction in p50 reload time</strong> and a ~<strong>60% reduction in p90 reload time</strong> for Instagram development servers.
At the same time, it virtually got rid of all import cycle error events we were seeing every day.
Other servers and tools consistently saw improvements between <strong>50% to 70%</strong> and memory usage reduction of <strong>20% to 40%</strong>.</p>

<p><img src="/img/posts/meta-cinder-lazy-import-img005.png" style="max-width:480px" />
<span class="caption text-muted">Can you guess when Lazy Imports was enabled in the graph?</span></p>

<p>See additional results <a href="https://github.com/facebookincubator/cinder/blob/cinder/3.8/CinderDoc/lazy_imports.rst#results">here</a>.</p>

<h2 id="challenges">Challenges</h2>

<p>Along the way, I ran into many obstacles, too many to list in this post.
Some were more complex than others, but all of them were interesting and challenging.
I can recall a couple bugs in CPython (<a href="https://bugs.python.org/issue41249">bpo-41249</a>, related to <code class="language-plaintext highlighter-rouge">TypedDict</code>), some libraries that I had to remove and a whole bunch of tests that I had to fix.</p>

<p>In my journey making codebases compatible with Lazy Imports, the problems that are more common when we start using Lazy Imports are:</p>

<ol>
  <li>Related to modules relying on <em>Import Side Effects</em>:
    <ul>
      <li>Code executing any logic when being imported.</li>
      <li>Relying on submodules being set as attributes in the parent modules.</li>
    </ul>
  </li>
  <li>Issues related to dynamic Python paths; particularly adding (and then removing after the import) paths from <code class="language-plaintext highlighter-rouge">sys.path</code>.</li>
  <li>All the errors are deferred from import time to first-use time (including <code class="language-plaintext highlighter-rouge">ModuleNotFoundError</code>), which might complicate debugging.</li>
  <li>Care should be taken when applying type annotations or it could inadvertently defeat Lazy Imports:
    <ul>
      <li>Modules should use <code class="language-plaintext highlighter-rouge">from __future__ import annotations</code>.</li>
      <li>We should use string type annotations for <code class="language-plaintext highlighter-rouge">typing.TypeVar()</code> and <code class="language-plaintext highlighter-rouge">typing.NewType()</code>.</li>
      <li>Wrap type aliases inside a <code class="language-plaintext highlighter-rouge">TYPE_CHECKING</code> conditional block.</li>
    </ul>
  </li>
</ol>

<p>For more comprehensive issues and gotchas, see <a href="https://github.com/facebookincubator/cinder/blob/cinder/3.8/CinderDoc/lazy_imports.rst#issues-and-gotchas">here</a>.</p>

<h2 id="highlights">Highlights</h2>

<p>Even though the <em>concept</em> of lazy imports is not entirely new and is conceptually simple (i.e., deferring module loading until imported names are used), we are not aware of any other low level implementation directly in CPython internals and none of the <a href="https://github.com/facebookincubator/cinder/blob/cinder/3.8/CinderDoc/lazy_imports.rst#prior-art">previous efforts</a> matches our current implementation in Cinder.
Some of its highlights are:</p>

<ul>
  <li>It provides an <em><strong>automatic</strong></em>, <em><strong>robust</strong></em> and mostly <em><strong>transparent</strong></em> solution to the often used paradigm of making things <em>lazy</em> in Python.</li>
  <li>It needs <em><strong>little effort</strong></em> to be used. We can turn Lazy Imports on <em>globally</em>, as a language level feature, and have Python <em>load every single module and package ever being used lazily</em> (even third party and standard library packages).</li>
  <li>It’s <em><strong>efficient</strong></em>. We ran a series of experiments in our live servers and results were performance neutral when adding the Lazy Imports patch (but not enabling the feature). We also ran the open source <a href="https://pyperformance.readthedocs.io/"><u>pyperformance</u></a> <strong>3 times</strong>, and observed the following most significant results when Lazy Imports is enabled vs. without the patch:
<img src="/img/posts/meta-cinder-lazy-import-img006.png" style="max-width:360px" /></li>
  <li><strong>No more import cycles</strong>. That doesn’t mean there can’t be circular imports with Lazy Imports enabled. There can still be legitimate cyclic dependencies at module level, but most cycles won’t be harmful and won’t manifest themselves as import errors. In our use case at Instagram, we went from engineers seeing ~80 circular import errors every day to zero.</li>
  <li>It <em><strong>Just Works™️</strong></em> (most of the time).</li>
</ul>

<h2 id="what-lies-ahead">What Lies Ahead</h2>

<ul>
  <li>With the right amount of warmup, Lazy Imports would for sure give us some gains in <em>memory usage</em>, <em>startup times</em> and perhaps (hopefully) even some <em>performance wins</em> in Instagram production servers.</li>
  <li>Not having to worry about <em>Circular Imports</em>, Lazy Imports opens a whole new avenue for modernizing and improving the quality of codebases. Refactoring becomes much easier and things that were once impossible are now feasible.</li>
  <li>Work with external third-party packages and libraries so that they are lazy-imports-friendly, making it possible for many more applications to take advantage of this capability.</li>
  <li><a href="https://peps.python.org/pep-0690/"><u>Upstreaming Lazy Imports to make it available to the broader Python ecosystem</u></a>!</li>
</ul>

<hr />

<p>References</p>

<ul>
  <li><a href="https://developers.facebook.com/blog/post/2022/06/15/python-lazy-imports-with-cinder/">Python Lazy Imports With Cinder</a></li>
</ul>]]></content><author><name>miintto</name></author><category term="meta engineering" /><category term="python" /><category term="cinder" /><category term="lazy import" /><summary type="html"><![CDATA[파이썬은 별도 컴파일 과정 없이 빠른 코드 수정 및 실행이 가능하다는 장점 덕분에 개발자 친화적인 언어로 널리 사용되고 있습니다. 하지만 대규모로 운영되는 인스타그램 서버를 로컬 개발 환경에 구성하는 경우 이러한 사용성 이점을 제대로 누리지 못하고 있었습니다.]]></summary></entry><entry><title type="html">[OpenSearch] 은전한닢 설치하기</title><link href="https://miintto.github.io/docs/opensearch-seunjeon" rel="alternate" type="text/html" title="[OpenSearch] 은전한닢 설치하기" /><published>2024-06-30T00:00:00+00:00</published><updated>2024-06-30T00:00:00+00:00</updated><id>https://miintto.github.io/docs/opensearch-seunjeon</id><content type="html" xml:base="https://miintto.github.io/docs/opensearch-seunjeon"><![CDATA[<p>저희 회사에서는 검색 엔진으로 엘라스틱서치를 사용하고 있습니다.
초창기에는 Elastic 클라우드 서비스를 활용하여 운영하였으나 인프라 아키텍처를 재편성하면서 <strong>AWS OpenSearch</strong>로 이관하였습니다.
개발기는 별도 AWS OpenSearch 도메인을 구성하는 대신 개발 인스턴스의 남는 자원을 활용하여 오픈서치를 직접 구동시키는 방법으로 운영하였습니다.</p>

<p>운영기에 맞추어 개발기의 버전도 1.2.4로 통일하였지만, 문제가 되었던 부분은 형태소 분석기였습니다.
AWS OpenSearch에서는 공식적으로 한글 분석을 위해 <strong>은전한닢(Seunjeon)</strong> 분석기를 지원하고 있습니다.
2023년부터 nori 분석기도 지원하고 있지만 <a href="https://docs.aws.amazon.com/opensearch-service/latest/developerguide/supported-plugins.html">공식 문서</a>에 따르면 1.3 버전부터 사용할 수 있게 되어있습니다.
반면 개발기의 경우 플러그인을 직접 설치해 주어야 하는데 <a href="https://bitbucket.org/eunjeon/seunjeon">공식 은전한닢 프로젝트</a>는 엘라스틱서치 6 버전까지만 지원하고 있어서 엘라스틱서치 7 버전이나 오픈서치에서는 직접 설치가 불가능한 상황이었습니다.</p>

<p>어쨋든 개발을 위해 개발 &amp; 운영 간 통일된 형태소 분석기를 구성하는 건 반드시 필요한 일이었습니다.
AWS에서 버젓이 은전한닢을 공식적으로 지원하는 걸 보면 오픈서치에 어떻게든 설치할 방법이 있지 않을까 하는 막연한 생각에 개발 오픈서치에 은전한닢 분석기를 설치하기로 했습니다.</p>

<hr />

<h1 id="1-plugin">1. Plugin</h1>

<p>엘라스틱서치에도 기본적으로 많은 기능이 있지만 다양한 기능과 확장성을 위해 부가적으로 다양한 플러그인을 지원하고 있습니다.
위에서 설명한 형태소 분석기 외에도 보안, 모니터링, 알림 등의 기능을 하는 여러 플러그인이 존재합니다.</p>

<p>동일한 기능을 하는 플러그인이더라도 엘라스틱서치의 버전에 따라 여러 버전이 존재할 수 있습니다.
엘라스틱서치의 버전이 올라가면서 플러그인과 호환성이 어그러질 수 있기 때문에 웬만하면 동일한 버전을 설치하는 걸 권장하고 있습니다.
예를 들어 엘라스틱서치 6.x 버전과 7.x 버전은 플러그인 디렉토리 구조가 달라서 서로 호환이 되지 않습니다.</p>

<p>아래와 같이 현재 설치된 플러그인과 버전을 조회할 수 있습니다.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>GET /_cat/plugins?v

name          component      version
60958e271a6b  analysis-icu   7.16.1
60958e271a6b  analysis-nori  7.16.1
...
</code></pre></div></div>

<h2 id="11-structure">1.1 Structure</h2>

<p>대다수의 플러그인은 ZIP 파일 형태로 배포되는데 기본적으로 아래와 같은 구조로 되어있습니다.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>custom-plugin-7.x.x.zip
├── custom-plugin-7.x.x.jar
└── plugin-descriptor.properties
</code></pre></div></div>

<p>JAR 파일은 플러그인의 주 실행 파일입니다.
플러그인의 주요 기능을 구성하는 Java 클래스 파일들이 포함되어 있습니다.</p>

<p>plugin-descriptor.properties 파일에는 메타데이터가 정의되어 있습니다.
아래와 같이 플러그인의 이름과 버전, 메인 클래스, 엘라스틱서치 호환 버전 등을 포함하고 있습니다.</p>

<div class="language-conf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">description</span>=<span class="n">The</span> <span class="n">Korean</span>(<span class="n">seunjeon</span>) <span class="n">analysis</span> <span class="n">plugin</span>.
<span class="n">version</span>=<span class="m">6</span>.<span class="m">1</span>.<span class="m">1</span>.<span class="m">0</span>
<span class="n">name</span>=<span class="n">analysis</span>-<span class="n">seunjeon</span>
<span class="n">classname</span>=<span class="n">org</span>.<span class="n">bitbucket</span>.<span class="n">eunjeon</span>.<span class="n">seunjeon</span>.<span class="n">elasticsearch</span>.<span class="n">plugin</span>.<span class="n">analysis</span>.<span class="n">AnalysisSeunjeonPlugin</span>
<span class="n">java</span>.<span class="n">version</span>=<span class="m">1</span>.<span class="m">8</span>
<span class="n">elasticsearch</span>.<span class="n">version</span>=<span class="m">6</span>.<span class="m">1</span>.<span class="m">1</span>
</code></pre></div></div>

<p>엘라스틱서치 6 이하 버전의 플러그인은 다른 구조로 되어있습니다.
구성 파일이 루트 디렉토리가 아닌 elasticsearch/ 디렉토리 하위에 위치해야 합니다.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>custom-plugin-6.x.x.zip
└── elasticsearch/
    ├── custom-plugin-6.x.x.jar
    └── plugin-descriptor.properties
</code></pre></div></div>

<h2 id="12-installation">1.2 Installation</h2>

<p>플러그인은 <code class="language-plaintext highlighter-rouge">elasticsearch-plugin</code> 명령어를 사용하여 설치할 수 있습니다.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 플러그인 다운로드</span>
<span class="nv">$&gt;</span> wget https://example.com/plugins/custom-plugin-7.16.1.zip
<span class="c"># 플러그인 설치</span>
<span class="nv">$&gt;</span> bin/elasticsearch-plugin <span class="nb">install </span>file:///path/custom-plugin-7.16.1.zip

-&gt; Installing file:///path/custom-plugin-7.16.1.zip
-&gt; Downloading file:///path/custom-plugin-7.16.1.zip
-&gt; Installed custom-plugin with folder name custom-plugin
</code></pre></div></div>

<p>플러그인 설치 과정에서 엘라스틱서치는 메타데이터(plugin-descriptor.properties) 파일을 읽고 <code class="language-plaintext highlighter-rouge">elasticsearch.version</code> 값이 버전과 일치하는지 검증합니다.
만일 버전이 일치하지 않으면 설치 과정에서 아래 에러가 발생합니다.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Exception <span class="k">in </span>thread <span class="s2">"main"</span> java.lang.IllegalArgumentException: Plugin <span class="o">[</span>custom-plugin] was built <span class="k">for </span>Elasticsearch version 6.1.1 but version 7.16.1 is running
    at org.elasticsearch.plugins.PluginsService.verifyCompatibility<span class="o">(</span>PluginsService.java:391<span class="o">)</span>
    at org.elasticsearch.plugins.cli.InstallPluginAction.loadPluginInfo<span class="o">(</span>InstallPluginAction.java:831<span class="o">)</span>
    at org.elasticsearch.plugins.cli.InstallPluginAction.installPlugin<span class="o">(</span>InstallPluginAction.java:887<span class="o">)</span>
</code></pre></div></div>

<p>만일 공식 플러그인이라면 레지스트리를 통해 직접 설치할 수 있습니다.
이 경우 엘라스틱서치 버전에 알맞은 플러그인을 자동으로 가져와 설치합니다.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$&gt;</span> bin/elasticsearch-plugin <span class="nb">install </span>analysis-icu
</code></pre></div></div>

<p>설치 후에는 엘라스틱서치를 재시작해 주어야 설치한 플러그인이 반영됩니다.</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$&gt;</span> systemctl restart elasticsearch
</code></pre></div></div>

<hr />

<h1 id="2-apply-to-opensearch">2. Apply to OpenSearch</h1>

<p>위 내용을 기반으로 오픈서치에 은전한닢 플러그인 설치를 진행했습니다.
기본적인 원리는 엘라스틱서치와 오픈서치가 동일합니다.
다만 설치 명령어로 <code class="language-plaintext highlighter-rouge">opensearch-plugin</code>을 사용하고, 버전 검증 과정에서 메타데이터의 <code class="language-plaintext highlighter-rouge">elasticsearch.version</code> 대신 <code class="language-plaintext highlighter-rouge">opensearch.version</code>을 확인하는 차이 정도입니다.</p>

<p>은전한닢 형태소 분석기는 공식적으로 6.1.1.1 버전까지 지원하고 있지만 다행히도 <a href="https://github.com/likejazz/seunjeon-elasticsearch-7">7 버전 이상에서도 호환 가능한 플러그인을 제공하는 프로젝트</a>를 발견하였습니다.
모든 버전이 아닌 7.9.1, 7.16.2 등과 같은 특정 버전만 지원하고 있지만, 프로젝트 내부에 plugin-descriptor.properties 파일의 엘라스틱서치 호환 버전을 임의로 변경해 주는 <a href="https://github.com/likejazz/seunjeon-elasticsearch-7/blob/master/elasticsearch/scripts/downloader.sh">스크립트</a>를 제공하고 있습니다.</p>

<p>해당 스크립트를 조금 변경하여 오픈서치의 버전에 알맞게 바꾸어주었습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/usr/bin/env bash</span>

<span class="nv">OPENSEARCH_VERSION</span><span class="o">=</span><span class="s2">"1.2.4"</span>
<span class="nv">PLUGIN_VERSION</span><span class="o">=</span><span class="s2">"7.9.1"</span>

<span class="nv">ZIP_NAME</span><span class="o">=</span><span class="s2">"analysis-seunjeon-</span><span class="k">${</span><span class="nv">PLUGIN_VERSION</span><span class="k">}</span><span class="s2">.zip"</span>
<span class="nv">TMP_DIR</span><span class="o">=</span><span class="s2">"/tmp/analysis-seunjeon"</span>
<span class="nb">mkdir</span> <span class="nt">-p</span> <span class="nv">$TMP_DIR</span>

<span class="c">########################################################################################################################</span>
<span class="c"># download zip</span>
<span class="nv">REMOTE_FILE_NAME</span><span class="o">=</span><span class="s2">"https://github.com/likejazz/seunjeon-elasticsearch-7/releases/download/</span><span class="k">${</span><span class="nv">PLUGIN_VERSION</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">ZIP_NAME</span><span class="k">}</span><span class="s2">"</span>
curl <span class="nt">-L</span> <span class="nt">-o</span> <span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span>/<span class="k">${</span><span class="nv">ZIP_NAME</span><span class="k">}</span> <span class="nv">$REMOTE_FILE_NAME</span>
<span class="k">if</span> <span class="o">[</span> <span class="s2">"</span><span class="nv">$?</span><span class="s2">"</span> <span class="nt">-ne</span> <span class="s2">"0"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span><span class="nb">echo</span> <span class="s2">"invalid path </span><span class="nv">$REMOTE_FILE_NAME</span><span class="s2">"</span>
    <span class="nb">exit </span>1
<span class="k">fi

</span><span class="nb">pushd</span> <span class="nv">$TMP_DIR</span>

<span class="c">########################################################################################################################</span>
<span class="c"># build properties file</span>
<span class="nv">PROPERTI_FILE</span><span class="o">=</span><span class="s2">"plugin-descriptor.properties"</span>

<span class="nb">cat</span> <span class="o">&gt;</span> <span class="nv">$PROPERTI_FILE</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh">
description=The Korean(seunjeon) analysis plugin.
version=</span><span class="k">${</span><span class="nv">PLUGIN_VERSION</span><span class="k">}</span><span class="sh">
name=analysis-seunjeon
classname=org.bitbucket.eunjeon.seunjeon.elasticsearch.plugin.analysis.AnalysisSeunjeonPlugin
java.version=1.8
opensearch.version=</span><span class="k">${</span><span class="nv">OPENSEARCH_VERSION</span><span class="k">}</span><span class="sh">
</span><span class="no">EOF

</span><span class="c">########################################################################################################################</span>
<span class="c"># zipping...</span>
zip <span class="nv">$ZIP_NAME</span> <span class="nv">$PROPERTI_FILE</span>
<span class="k">if</span> <span class="o">[</span> <span class="s2">"</span><span class="nv">$?</span><span class="s2">"</span> <span class="nt">-ne</span> <span class="s2">"0"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span><span class="nb">exit </span>1
<span class="k">fi

</span><span class="nb">popd</span>

<span class="c">########################################################################################################################</span>
<span class="c"># copy a plugin file to current directory.</span>
<span class="nb">cp</span> <span class="nv">$TMP_DIR</span>/<span class="nv">$ZIP_NAME</span> <span class="nb">.</span>
</code></pre></div></div>

<p>개발 오픈서치는 도커 기반으로 구성하였는데 아래와 같이 변화를 주었습니다.
빌드 이미지를 별도 분리하여 다운받은 플러그인을 오픈서치 1.2.4 버전으로 태깅하고, 런타임에서는 플러그인 ZIP 파일을 가져와 설치하는 방식으로 구성하였습니다.</p>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="w"> </span><span class="s">ubuntu:latest</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="s">build</span>
<span class="k">WORKDIR</span><span class="s"> /app/build</span>
<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> apt-get <span class="nt">-y</span> upgrade <span class="se">\
</span>    <span class="o">&amp;&amp;</span> apt-get <span class="nb">install</span> <span class="nt">-y</span> curl zip
<span class="k">COPY</span><span class="s"> ./scripts/downloads.sh .</span>
<span class="k">RUN </span>bash downloads.sh

<span class="k">FROM</span><span class="w"> </span><span class="s">opensearchproject/opensearch:1.2.4</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="s">runtime</span>
<span class="k">WORKDIR</span><span class="s"> /usr/share/opensearch</span>
<span class="k">COPY</span><span class="s"> --from=build /app/build/analysis-seunjeon-7.9.1.zip .</span>
<span class="k">RUN </span>/usr/share/opensearch/bin/opensearch-plugin <span class="nb">install</span> <span class="nt">--batch</span> file://<span class="sb">`</span><span class="nb">pwd</span><span class="sb">`</span>/analysis-seunjeon-7.9.1.zip
</code></pre></div></div>

<p>이후 도커 이미지를 빌드해보았지만 아래 에러가 발생하며 실패하고 말았습니다.</p>

<pre><code class="language-txt">org.opensearch.bootstrap.StartupException: OpenSearchException[Unable to load plugin class [org.bitbucket.eunjeon.seunjeon.elasticsearch.plugin.analysis.AnalysisSeunjeonPlugin]]; nested: NoClassDefFoundError[org/elasticsearch/plugins/AnalysisPlugin];
    at org.opensearch.bootstrap.OpenSearch.init(OpenSearch.java:182) ~[opensearch-1.2.4.jar:1.2.4]
    at org.opensearch.bootstrap.OpenSearch.execute(OpenSearch.java:169) ~[opensearch-1.2.4.jar:1.2.4]
    at org.opensearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:100) ~[opensearch-1.2.4.jar:1.2.4]
</code></pre>

<p>아무래도 엘라스틱서치 플러그인을 오픈서치에 그대로 가져다가 설치해서 발생한 오류로 보여집니다.</p>

<hr />

<h1 id="3-build-with-sbt">3. Build with sbt</h1>

<p>다른 방안을 물색하던 끝에 은전한닢을 오픈서치에 적용 가능하도록 패치한 프로젝트 <a href="https://bitbucket.org/soosinha/seunjeon-opensearch">seunjeon-opensearch</a> 를 발견했습니다.
공식 은전한닢 repo를 포크하여 오픈서치 버전으로 리팩토링한 프로젝트였습니다.
다만 별도 ZIP 파일을 내려받을 레지스트리가 없는 것 같아서 플러그인을 직접 빌드해야만 했습니다.</p>

<p>기본적으로 은전한닢은 <strong>sbt</strong>를 이용하여 빌드하도록 되어있습니다.
sbt란 Simple Build Tool의 약자로 Java와 Scalar로 작성된 프로젝트를 빌드하기 위한 도구입니다.
build.sbt 파일에서 프로젝트의 빌드 과정을 정의하며 <code class="language-plaintext highlighter-rouge">libraryDependencies</code> 에서 의존성을 관리합니다.</p>

<p>빌드를 위해 JDK 1.8과 스칼라 2.12 환경이 필요했는데 로컬 환경에 구성하기는 번거로울 것 같아 도커 이미지를 빌드하였습니다.
빌드 이미지에서 sbt 환경 구성 및 플러그인 빌드를 진행하고 opensearch/target/ 경로에 생성된 ZIP 파일만 런타임 이미지로 가져와 플러그인을 설치하도록 구성하였습니다.</p>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="w"> </span><span class="s">openjdk:8</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="s">build</span>
<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> apt-get <span class="nb">install</span> <span class="nt">-y</span> curl git wget <span class="se">\
</span>    <span class="o">&amp;&amp;</span> <span class="nb">echo</span> <span class="s2">"deb https://repo.scala-sbt.org/scalasbt/debian /"</span> | <span class="nb">tee</span> <span class="nt">-a</span> /etc/apt/sources.list.d/sbt.list <span class="se">\
</span>    <span class="o">&amp;&amp;</span> curl <span class="nt">-sL</span> <span class="s2">"https://keyserver.ubuntu.com/pks/lookup?op=get&amp;search=0x2EE0EA64E40A89B84B2DF73499E82A75642AC823"</span> | apt-key add <span class="se">\
</span>    <span class="o">&amp;&amp;</span> apt-get update <span class="o">&amp;&amp;</span> apt-get <span class="nb">install</span> <span class="nt">-y</span> sbt
<span class="k">WORKDIR</span><span class="s"> /app</span>
<span class="k">RUN </span>git clone https://bitbucket.org/soosinha/seunjeon-opensearch.git
<span class="k">WORKDIR</span><span class="s"> /app/seunjeon-opensearch</span>
<span class="k">RUN </span><span class="nb">echo</span> <span class="s2">"addSbtPlugin(</span><span class="se">\"</span><span class="s2">com.jsuereth</span><span class="se">\"</span><span class="s2"> % </span><span class="se">\"</span><span class="s2">sbt-pgp</span><span class="se">\"</span><span class="s2"> % </span><span class="se">\"</span><span class="s2">1.1.0</span><span class="se">\"</span><span class="s2">)"</span> <span class="o">&gt;&gt;</span> ./project/plugins.sbt <span class="se">\
</span>    <span class="o">&amp;&amp;</span> sbt update
<span class="k">RUN </span><span class="nb">sed</span> <span class="nt">-i</span> <span class="s1">'s/val opensearchVersion = "1.0.0"/val opensearchVersion = "1.2.4"/'</span> build.sbt <span class="se">\
</span>    <span class="o">&amp;&amp;</span> <span class="nb">sed</span> <span class="nt">-i</span> <span class="s1">'s/val opensearchJarVersion = "1.0.0-beta1"/val opensearchJarVersion = "1.0.0"/'</span> build.sbt <span class="se">\
</span>    <span class="o">&amp;&amp;</span> bash ./scripts/download-dict.sh mecab-ko-dic-2.0.1-20150920 <span class="se">\
</span>    <span class="o">&amp;&amp;</span> sbt <span class="nt">-J-Xmx2G</span> <span class="s2">"runMain org.bitbucket.eunjeon.seunjeon.DictBuilder"</span> <span class="se">\
</span>    <span class="o">&amp;&amp;</span> sbt <span class="s2">"project opensearch"</span> <span class="s2">"opensearchZip"</span>

<span class="k">FROM</span><span class="w"> </span><span class="s">opensearchproject/opensearch:1.2.4</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="s">runtime</span>
<span class="k">WORKDIR</span><span class="s"> /usr/share/opensearch</span>
<span class="k">COPY</span><span class="s"> --from=build /app/seunjeon-opensearch/opensearch/target/opensearch-analysis-seunjeon-assembly-1.2.4.zip .</span>
<span class="k">RUN </span>/usr/share/opensearch/bin/opensearch-plugin <span class="nb">install</span> <span class="nt">--batch</span> file://<span class="sb">`</span><span class="nb">pwd</span><span class="sb">`</span>/opensearch-analysis-seunjeon-assembly-1.2.4.zip
</code></pre></div></div>

<p>도커 이미지에서도 sbt 빌드 환경을 구성하는 건 꽤나 까다로운 일이었습니다.
프로젝트 README 에서 어느 정도 빌드 가이드를 제공하고 있었지만, sbt-pgp 플러그인이 누락되어 추가로 구성하거나 버전 태깅 같은 부분에 대해 약간의 수정이 필요했습니다.</p>

<p>빌드 후 오픈서치를 실행해 보면 은전한닢 형태소 분석기가 설치된 것을 확인할 수 있습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>GET /_cat/plugins?v

name             component                     version
opensearch-node  analysis-seunjeon             1.2.4
opensearch-node  opensearch-alerting           1.2.4.0
opensearch-node  opensearch-anomaly-detection  1.2.4.0
...
</code></pre></div></div>

<hr />

<p>References</p>

<ul>
  <li><a href="https://www.elastic.co/guide/en/elasticsearch/plugins/current/plugin-management.html">Plugin management | Elastic</a></li>
  <li><a href="https://bitbucket.org/eunjeon/seunjeon">eunjeon / seunjeon — Bitbucket</a></li>
  <li><a href="https://github.com/likejazz/seunjeon-elasticsearch-7">GitHub - likejazz/seunjeon-elasticsearch-7</a></li>
  <li><a href="https://docs.likejazz.com/seunjeon-elasticsearch-7/">엘라스틱서치 7을 위한 은전한닢 형태소 분석기 · The Missing Papers</a></li>
  <li><a href="https://bitbucket.org/soosinha/seunjeon-opensearch">soosinha / seunjeon-opensearch — Bitbucket</a></li>
  <li><a href="https://groups.google.com/g/eunjeon/c/5P4ekbXb5O8">seunjeon package build 진행 시 오류</a></li>
</ul>]]></content><author><name>miintto</name></author><category term="elasticsearch" /><category term="opensearch" /><category term="plugin" /><category term="sbt" /><category term="seunjeon" /><summary type="html"><![CDATA[저희 회사에서는 검색 엔진으로 엘라스틱서치를 사용하고 있습니다. 초창기에는 Elastic 클라우드 서비스를 활용하여 운영하였으나 인프라 아키텍처를 재편성하면서 AWS OpenSearch로 이관하였습니다. 개발기는 별도 AWS OpenSearch 도메인을 구성하는 대신 개발 인스턴스의 남는 자원을 활용하여 오픈서치를 직접 구동시키는 방법으로 운영하였습니다.]]></summary></entry><entry><title type="html">[번역] Python 3.12에 대한 Meta의 공헌</title><link href="https://miintto.github.io/docs/meta-contribution-for-python312" rel="alternate" type="text/html" title="[번역] Python 3.12에 대한 Meta의 공헌" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://miintto.github.io/docs/meta-contribution-for-python312</id><content type="html" xml:base="https://miintto.github.io/docs/meta-contribution-for-python312"><![CDATA[<blockquote>
  <p>해당 포스트는 Meta Engineering 블로그의 <a href="https://engineering.fb.com/2023/10/05/developer-tools/python-312-meta-new-features/">Meta contributes new features to Python 3.12</a> 포스트를 번역한 글입니다.</p>

  <p>게시일: 2023.10.05</p>
</blockquote>

<h1 id="메타가-python-312에-도입한-기능">메타가 Python 3.12에 도입한 기능</h1>

<p>이번 <a href="https://discuss.python.org/t/python-3-12-0-final-is-here/35186">파이썬 3.12</a> 릴리즈에는 우리 내부의 사용 사례를 파이썬 커뮤니티에서 더 쉽게 접근할 수 있도록 개발 및 확장하였던 메타의 노력이 담겨있습니다.
이처럼 메타를 구성하는 오픈소스는 우리의 작업 방식과 학습한 결과를 커뮤니티에 공유하는 아주 중요한 역할을 합니다.</p>

<p>몇 년 동안 우리는 자체적인 Python 런타임 <a href="https://github.com/facebookincubator/cinder">Cinder</a>를 공개하고 있습니다.
또한 파이썬 커뮤니티와 꾸준히 밀접하게 협력하고 있는데, 새로운 기능과 최적화를 도입하여 파이썬 성능을 향상시키고 제삼자 입장에서 좀 더 수월하게 파이썬 런타임 최적화를 진행하도록 지원하고 있습니다.</p>

<p>이번 파이썬 3.12 릴리즈에서 우리는 아래와 같은 여러 부문에서 기능을 제안하고 구현하였습니다.</p>

<ol>
  <li>불멸 객체</li>
  <li>Type 시스템 고도화</li>
  <li>성능 최적화</li>
  <li>새로운 벤치마크 추가</li>
  <li>Cinder Hooks</li>
</ol>

<h1 id="1-불멸-객체">1. 불멸 객체</h1>

<p><a href="https://peps.python.org/pep-0683/">Immortal Objects – PEP 683</a>에서는 레퍼런스 카운팅에 개입하지 않고 파이썬 인터프리터가 종료될 때까지 사라지지 않는 일명 <strong>불멸 객체</strong>(Immortal Objects)의 개념을 수용하였습니다.
처음 이러한 방식을 도입한 계기는 인스타그램 웹 서버의 메모리 워크로드를 최적화 하기 위해서였는데, 해당 방식으로 레퍼런스 카운팅으로 인한 copy-on-write를 줄이면서 성능을 이끌어 냈었습니다.</p>

<p>불멸 객체의 도입은 여러 파이썬 인터프리터 사이에 공유되는 파이썬 불변 객체(Immutable Object)에 대한 locking 절차(예를 들면 GIL)가 더 이상 필요 없다는 의미에서도 중요합니다.
이를 활용하여 <a href="https://peps.python.org/pep-0684/">여러 서브 인터프리터</a> 혹은 <a href="https://peps.python.org/pep-0703/">GIL 없는 멀티스레딩</a>과 같은 방식으로 파이썬의 싱글 프로세스 병렬 처리 성능을 개선할 수 있습니다.</p>

<h1 id="2-type-시스템-고도화">2. Type 시스템 고도화</h1>

<p>파이썬 타입 체킹 오픈소스 <a href="https://pyre-check.org/">Pyre</a>의 엔지니어링 팀에서는 <code class="language-plaintext highlighter-rouge">@type.override</code> 데코레이터를 추가하였는데, 클래스 상속 계층을 리팩토링하는 과정에서 매소드 오버라이딩 관련 버그를 방지하는 데 도움을 주는 기능을 합니다.</p>

<p>사용자들은 이러한 데코레이터를 하위 클래스의 상속받는 메소드에 적용할 수 있습니다.
그리고 상위 클래스의 메소드를 건드리면서 상속받은 메소드가 더 이상 존재하지 않는 경우 경고 신호를 주는데, 이렇게 클래스 간의 메소드 상속 관계가 깨지는 걸 방지할 수 있습니다.
이런 방식으로 리팩토링의 신뢰성을 더해주고 좀 더 수월하게 코드를 유지보수 할 수 있습니다.</p>

<h1 id="3-성능-최적화">3. 성능 최적화</h1>

<h2 id="더-빨라진-comprehensions">더 빨라진 Comprehensions</h2>

<p>이전 파이썬 버전에서 <strong>comprehension</strong>은 내부의 매 실행마다 파이썬 함수 객체를 할당하고 지우는 작업을 반복하면서 마치 nested 함수처럼 컴파일되고 있었습니다.</p>

<p>파이썬 3.12에서 <a href="https://peps.python.org/pep-0709/">PEP 709</a>를 도입하면서 list, dict, set comprehension에 대해 기존보다 최대 두 배까지 성능을 높일 수 있었습니다.</p>

<p>추가적으로 해당 기능을 개발하면서 파이썬 3.11에서 잘못된 동작을 유발할 수 있는 기존 바이트코드 컴파일러 버그도 발견하여 <a href="https://github.com/python/cpython/pull/104620">수정</a>하였습니다.</p>

<h2 id="즉각적인-asyncio-태스크">즉각적인 asyncio 태스크</h2>

<p>파이썬의 비동기 작업이 단일 프로세스에서 동시성을 제공하고 있지만, 이 과정에서 많은 런타임 오버헤드가 발생하고 있었습니다.
비동기 함수를 호출할 때마다 별개의 코루틴 객체가 생성되고, 표준 asyncio 라이브러리가 추가적으로 <a href="https://docs.python.org/3.12/library/asyncio-task.html#asyncio.Task">태스크(Task)</a> 객체와 이벤트 루프(Event Loop) 스케줄링 작업을 하면서 많은 부하를 유발할 수 있습니다.</p>

<p>하지만 우리 내부의 모든 비동기 코드베이스를 조사해 보았더니 몇몇 상황에서는 굳이 지연할 필요 없이 바로 결과를 반환할 수 있다는 사실을 발견했습니다.
이런 상황에서 함수의 결과값을 곧바로 반환하게 된다면 코루틴 혹은 태스크 객체와 이벤트 루프 스케줄링 작업이 불필요한 오버헤드가 될 수 있습니다.</p>

<p>Cinder에서는 즉각적으로 비동기 실행 방식으로 이러한 오버헤드를 제거하였습니다.
만약 비동기 함수 호출이 곧바로 결과값을 가져오게 된다면 별도 코루틴 객체를 생성하지 않고 그 결과를 반환합니다.
<a href="https://docs.python.org/3.12/library/asyncio-task.html#asyncio.gather"><code class="language-plaintext highlighter-rouge">asyncio.gather()</code></a> 함수를 사용하는 경우에도 매 비동기 함수들이 즉시 결과를 반환할 수 있다면 태스크 생성이나 이벤트 루프 스케줄링 과정이 생략됩니다.</p>

<p>이러한 즉각적인 비동기 실행 방식은 워낙 파격적인 코드 변경이 많아서 불행히도 파이썬 3.11부터 도입된 <a href="https://docs.python.org/3.12/library/asyncio-task.html#asyncio.TaskGroup">TaskGroup</a> API 환경에서는 제대로 작동하지 않았습니다.
대신 파이썬 3.12에서는 <a href="https://docs.python.org/3.12/library/asyncio-task.html#asyncio.eager_task_factory"><strong>eager asyncio tasks</strong></a>라는 좀 더 간단한 버전으로 추가하였습니다.
Eager task에서는 여전히 결과값을 즉시 반환할 수 있는 경우에도 코루틴과 테스크를 생성하게 되어있지만, 경우에 따라 이벤트 루프에 스케줄링하는 과정을 건너뛰도록 하였습니다.</p>

<p>이러한 방식은 매우 효율적이지만 정확한 명칭으로는 “커스텀 task factory를 통한 선택적 태스크 실행”에 더 가깝습니다.</p>

<h2 id="기타-비동기-개선">기타 비동기 개선</h2>

<p>다른 비동기 관련 작업으로 더 향상된 속도의 <a href="https://github.com/python/cpython/pull/100345">asyncio.current_task의 C 구현체</a>와 <a href="https://github.com/python/cpython/pull/103767">비동기 태스크 생성 최적화</a> 그리고 <a href="https://github.com/python/cpython/pull/103767#issuecomment-1528900046">비동기 벤치마크에서 최대 5%의 성능 향상</a>도 있습니다.</p>

<h2 id="향상된-super-호출-속도">향상된 <code class="language-plaintext highlighter-rouge">super()</code> 호출 속도</h2>

<p>새롭게 추가한 <a href="https://docs.python.org/3.12/library/dis.html#opcode-LOAD_SUPER_ATTR">LOAD_SUPER_ATTR opcode</a>로 <code class="language-plaintext highlighter-rouge">super().attr</code>와 <code class="language-plaintext highlighter-rouge">super().method(…)</code> 형태의 코드 성능을 최적화하였습니다.
이전에는 코드를 실행할 때마다 일회용 “super” 객체의 할당과 제거 과정이 반드시 필요했습니다.
이제는 일반적인 메서드나 인자 호출보다도 오버헤드가 거의 발생하지 않습니다.</p>

<h2 id="기타-성능-최적화">기타 성능 최적화</h2>

<p>그 외에도 추가적으로 두 가지 작업 <a href="https://github.com/python/cpython/pull/104063">hasattr</a><a href="https://github.com/python/cpython/pull/104079"> 최적화</a>와 <a href="https://github.com/python/cpython/pull/100252">unittest.mock.Mock 성능 3.8배 향상</a>도 있습니다.</p>

<h1 id="4-새로운-벤치마크-추가">4. 새로운 벤치마크 추가</h1>

<p>메타에서 내부적으로 파이썬을 최적화하는 경우에는 보통 실제 운영 환경에 대비하여 최적화를 직접 테스트하고 유효성을 검증할 수 있습니다.
반면 오픈소스 파이썬 최적화 작업은 운영 환경을 위한 테스트 환경이 별도로 존재하지 않으며, 다양한 환경에서도 동일하게 효율적인 성능을 내야만 합니다.</p>

<p><a href="https://github.com/python/pyperformance">파이썬 성능 벤치마크</a>는 파이썬 성능 최적화에 사용되는 표준 벤치마크입니다.
파이썬 3.12를 개발하면서 몇몇 새로운 벤치마크를 추가했는데, 메타에서 발견한 워크로드 특성이 정확하게 표현되어 있습니다.</p>

<p>우리가 추가한 기능은 아래와 같습니다.</p>

<ul>
  <li>과도한 asyncio 부하에 최적화된 <a href="https://github.com/python/pyperformance/pull/187"><u>async_tree 벤치마크 모델</u></a></li>
  <li>기존에 사각지대였던 <a href="https://github.com/python/pyperformance/pull/265"><u>comprehension</u></a>과 <a href="https://github.com/python/pyperformance/pull/271"><u>super()</u></a>를 더 철저하게 검증하는 두 개의 벤치마크</li>
</ul>

<h1 id="5-cinder-hooks">5. Cinder Hooks</h1>

<p>Cinder의 몇몇 부분(JIT 컴파일러와 Static 파이썬)은 플랫폼 호환 이슈, C와 C++의 차이, 의미 변경, 코드 사이즈와 같은 이유로 CPython에 접목시키기 까다로웠는데, 대신 우리는 독립적인 확장 모듈 CinderX로 패키징하는 방식을 택하였습니다.</p>

<p>이에 따라 코어 런타임에 여러 개의 새로운 hook이 필요해졌으며, 파이썬 3.12에 아래와 같은 hook을 추가해 두었습니다.</p>

<ul>
  <li><a href="https://github.com/python/cpython/pull/92257"><u>파이썬 함수에 대한 벡터콜 진입점 설정 API</u></a>. 해당 API는 JIT에게 주어진 함수에 대한 실행을 이어갈 시작 지점을 제공합니다.</li>
  <li><a href="https://github.com/python/cpython/pull/31787"><u>딕셔너리</u></a>, <a href="https://github.com/python/cpython/pull/97875"><u>타입</u></a>, <a href="https://github.com/python/cpython/pull/98175"><u>함수</u></a>, <a href="https://github.com/python/cpython/pull/99859"><u>코드 객체</u></a>를 감시하는 기능이 추가되었습니다. 이러한 기능들은 Cinder JIT의 예측을 넘어서는 동적 변화를 감지하고 가능한 빠른 경로를 유지하도록 합니다.</li>
  <li><a href="https://github.com/python/cpython/pull/102022"><u>CPython 코어 인터프리터의 코드 제너레이터 확장성</u></a>이 추가되어 static 파이썬이 추가된 정적 파이썬 opcode로 인터프리터를 쉽게 재생성할 수 있게 하였습니다. 또한 <a href="https://github.com/python/cpython/pull/102014"><u>모든 GC 추적 객체를 찾아가는 C API</u></a>를 활용하여 Cinder JIT가 활성화되기 전에 생성된 함수를 발견할 수 있게 되었습니다.ㄴ</li>
  <li><a href=""><u>Perf-map 파일에 thread-safe하게 접근하는 API</u></a> 또한 추가하였습니다. Perf-map 파일은 리눅스 perf 프로파일러이 기계어에서 동적으로 생성된 섹션에 사람이 읽을 수 있는 이름을 설정할 수 있도록 합니다. 이러한 API를 통해 Cinder JIT가 별개의 JIT 혹은 파이썬 3.12의 <a href="https://github.com/python/cpython/pull/96123"><u>perf trampoline</u></a>과 충돌 없이 안전하게 perf map 파일에 기록할 수 있도록 하였습니다.</li>
</ul>

<p>이러한 기능들은 CPython 써드파티 JIT 컴파일러나 런타임 옵티마이저를 제작하려는 모두에게 유용할 것이라고 기대합니다.
또한 추후 코어 CPython 내부 감시 기능을 활용할 계획도 있습니다.</p>

<h1 id="파이썬-312-그-이후">파이썬 3.12 그 이후</h1>

<p>파이썬은 메타에서 상당한 비중을 차지하고 있습니다.
<a href="/docs/python-immortal-objects">인스타그램의 서버 스택</a>을 포함하여 메타 내부 인프라의 주요한 구성 요소 중 하나입니다.
또한 파이썬은 <a href="https://ai.meta.com/blog/code-llama-large-language-model-coding/">AI/ML 작업</a>에서 가장 보편적으로 사용되는 언어로, 컴퓨터 비전이나 자연어 처리 등과 같이 광범위한 사용 사례를 위한 머신러닝 프레임워크 <a href="https://pytorch.org/">PyTorch</a>의 발전과 함께 주목받고 있습니다.</p>

<p>우리가 파이썬 커뮤니티에 기여하는 부분은 단지 3.12 릴리즈에만 그치지 않습니다.
현재는 <a href="https://peps.python.org/pep-0703/">PEP-703</a> 제안에 따라 파이썬에서 GIL을 걷어내고 여러 스레드에서 병렬로 실행시킬 방법을 고민하고 있습니다.
이러한 업데이트는 멀티스레딩 환경에서 파이썬을 사용하는 모두가 반길겁니다.</p>

<p>메타와 파이썬 커뮤니티 간의 관계도 여전히 계속됩니다.
2023년에도 꾸준히 <a href="https://pyfound.blogspot.com/2022/03/meta-deepens-its-investment-in-python.html">파이썬 Developer-in-Residence 프로그램</a>이나 <a href="https://us.pycon.org/2023/#">파이콘 US</a>와 같은 단체를 서포트하고 있습니다.
또한 파이콘에서 발표한 <a href="https://us.pycon.org/2023/schedule/presentation/155/">PyTorch의 파이썬 컴파일러를 활용한 AI/ML 성능 향상</a>과 <a href="https://engineering.fb.com/?s=python">메타 엔지니어링 블로그 게시글</a>을 통해 우리가 쌓은 지식을 공유하고 있습니다.</p>

<p>우리는 이러한 오픈소스 커뮤니티의 일원이 되어 항상 감사하고 있으며 함께 힘을 모아 파이썬 언어를 더 발전시키기를 기대합니다.</p>

<hr />

<h1 id="meta-contributes-new-features-to-python-312">Meta contributes new features to Python 3.12</h1>

<p>This week’s release of <a href="https://discuss.python.org/t/python-3-12-0-final-is-here/35186">Python 3.12</a> marks a milestone in our efforts to make our work developing and scaling Python for Meta’s use cases <a href="https://discuss.python.org/t/making-cinder-more-broadly-available/14062">more accessible to the broader Python community</a>.
Open source at Meta is an important part of how we work and share our learnings with the community.</p>

<p>For several years, we have been sharing our work on Python and CPython through our open source Python runtime, <a href="https://github.com/facebookincubator/cinder">Cinder</a>.
We have also been working closely with the Python community to introduce new features and optimizations to improve Python’s performance and to allow third parties to experiment with Python runtime optimization more easily.</p>

<p>For the Python 3.12 release, we proposed and implemented features in several areas:</p>

<ul>
  <li>Immortal Objects</li>
  <li>Type system improvements</li>
  <li>Performance optimizations</li>
  <li>New benchmarks</li>
  <li>Cinder hooks</li>
</ul>

<h1 id="immortal-objects">Immortal Objects</h1>

<p><a href="https://peps.python.org/pep-0683/">Immortal Objects – PEP 683</a> makes it possible to create Python objects that don’t participate in <a href="https://devguide.python.org/internals/garbage-collector/">reference counting</a>, and will live until Python interpreter shutdown.
The original motivation for this feature was to reduce memory use in the forking Instagram web-server workload by reducing copy-on-writes triggered by reference-count updates.</p>

<p>Immortal Objects are also an important step towards truly immutable Python objects that can be shared between Python interpreters with no need for locking, for example, via the global interpreter lock (GIL)
This can enable improved Python single-process parallelism, whether via <a href="https://peps.python.org/pep-0684/">multiple sub-interpreters</a> or <a href="https://peps.python.org/pep-0703/">GIL-free multi-threading</a>.</p>

<h1 id="type-system-improvements">Type system improvements</h1>

<p>The engineering team behind <a href="https://pyre-check.org/">Pyre</a>, an open source Python type-checker, authored and implemented <a href="https://peps.python.org/pep-0698/">PEP 698</a> to add a <code class="language-plaintext highlighter-rouge">@typing.override</code> decorator, which helps avoid bugs when refactoring class inheritance hierarchies that use method overriding.</p>

<p>Python developers can apply this new decorator to a subclass method that overrides a method from a base class.
As a result, static type checkers will be able to warn developers if the base class is modified such that the overridden method no longer exists.
Developers can avoid accidentally turning a method override into dead code.
This improves confidence in refactoring and helps keep the code more maintainable.</p>

<h1 id="performance-optimizations">Performance optimizations</h1>

<h2 id="faster-comprehensions">Faster comprehensions</h2>

<p>In previous Python versions, all comprehensions were compiled as nested functions, and every execution of a comprehension allocated and destroyed a single-use Python function object.</p>

<p>In Python 3.12, <a href="https://peps.python.org/pep-0709/">PEP 709</a> inlines all list, dict, and set comprehensions for better performance (up to two times better in the best case).</p>

<p>The implementation and debugging of PEP 709 also uncovered a pre-existing bytecode compiler bug that could result in silently wrong code execution in Python 3.11, which we <a href="https://github.com/python/cpython/pull/104620">fixed</a>.</p>

<h2 id="eager-asyncio-tasks">Eager asyncio tasks</h2>

<p>While Python’s asynchronous programming support enables single-process concurrency, it also has noticeable runtime overhead.
Every call to an async function creates an extra coroutine object, and the standard asyncio library will often bring additional overhead in the form of <a href="https://docs.python.org/3.12/library/asyncio-task.html#asyncio.Task">Task</a> objects and event loop scheduling.</p>

<p>We observed that, in practice, in a fully async codebase, many async functions are often able to return a result immediately, with no need to suspend.
In these cases, if the result of the function is immediately awaited, the coroutine/Task objects and event loop scheduling can be unnecessary overhead.</p>

<p>Cinder eliminates this overhead via eager async execution.
If an async function call is awaited immediately, it may return a result directly, without creating a coroutine object.
If an <a href="https://docs.python.org/3.12/library/asyncio-task.html#asyncio.gather"><code class="language-plaintext highlighter-rouge">asyncio.gather()</code></a> is immediately awaited, and all the async functions it gathers are able to return immediately, there’s no need to ever create a Task  or schedule it to the event loop.</p>

<p>Fully eager async execution would be an invasive (and breaking) change to Python, and doesn’t work as well with the new Python 3.11+ <a href="https://docs.python.org/3.12/library/asyncio-task.html#asyncio.TaskGroup">TaskGroup</a> API for managing concurrent tasks.
So in Python 3.12 we added a simpler version of the feature: <a href="https://docs.python.org/3.12/library/asyncio-task.html#asyncio.eager_task_factory">eager asyncio tasks</a>.
With eager tasks, coroutine and Task objects are still created when a result is available immediately, but we can sometimes avoid scheduling the task to the event loop and instead resolve it right away.</p>

<p>This is more efficient, but it is a semantic change, so this feature is <a href="https://docs.python.org/3.12/library/asyncio-task.html#asyncio.eager_task_factory">opt-in via a custom task factory</a>.</p>

<h2 id="other-asyncio-improvements">Other asyncio improvements</h2>

<p>We also landed a faster <a href="https://github.com/python/cpython/pull/100345">C implementation of asyncio.current_task</a> and an <a href="https://github.com/python/cpython/pull/103767">optimization to async task creation</a> that shows a <a href="https://github.com/python/cpython/pull/103767#issuecomment-1528900046">win of up to 5 percent on asyncio benchmarks</a>.</p>

<h2 id="faster-super-calls">Faster <code class="language-plaintext highlighter-rouge">super()</code> calls</h2>

<p>The new <a href="https://docs.python.org/3.12/library/dis.html#opcode-LOAD_SUPER_ATTR">LOAD_SUPER_ATTR opcode</a> optimizes code of the form <code class="language-plaintext highlighter-rouge">super().attr</code> and <code class="language-plaintext highlighter-rouge">super().method(…)</code>.
Such code previously had to allocate, and then throw away, a single-use “super” object each time it ran.
Now it has little more overhead than an ordinary method call or attribute access.</p>

<h2 id="other-performance-optimizations">Other performance optimizations</h2>
<p>We also landed two <a href="https://github.com/python/cpython/pull/104063">hasattr</a> <a href="https://github.com/python/cpython/pull/104079">optimizations</a> and a <a href="https://github.com/python/cpython/pull/100252">3.8x performance improvement to unittest.mock.Mock</a>.</p>

<h1 id="new-benchmarks">New benchmarks</h1>

<p>When we optimize Python for internal use at Meta, we are usually able to test and validate our optimizations directly against our real-world workloads.
Optimization work on open-source Python doesn’t have such a production workload to test against and needs to be effective (and avoid regression) on a variety of different workloads.</p>

<p>The <a href="https://github.com/python/pyperformance">Python Performance Benchmark suite</a> is the standard set of benchmarks used in open-source Python optimization work.
During the 3.12 development cycle, we contributed several new benchmarks to it so that it more accurately represents workload characteristics we see at Meta.</p>

<p>We added:</p>

<ul>
  <li>A <a href="https://github.com/python/pyperformance/pull/187"><u>set of async_tree benchmarks</u></a> that better model an asyncio-heavy workload.</li>
  <li>A pair of benchmarks that exercise <a href="https://github.com/python/pyperformance/pull/265"><u>comprehensions</u></a> and <a href="https://github.com/python/pyperformance/pull/271"><u>super()</u></a> more thoroughly, which were blind spots of the existing benchmark suite.</li>
</ul>

<h1 id="cinder-hooks">Cinder hooks</h1>

<p>Some parts of Cinder (our <a href="https://github.com/facebookincubator/cinder#the-cinder-jit">JIT compiler</a> and <a href="https://github.com/facebookincubator/cinder#static-python">Static Python</a>) wouldn’t make sense as part of upstream CPython (because of limited platform support, C versus C++, semantic changes, and just the size of the code), so our goal is to package these as an independent extension module, CinderX.</p>

<p>This requires a number of new hooks in the core runtime.
We landed many of these hooks in Python 3.12:</p>

<ul>
  <li>An <a href="https://github.com/python/cpython/pull/92257"><u>API to set the vectorcall entrypoint for a Python function</u></a>. This gives the JIT an entry point to take over execution for a given function.</li>
  <li>We added <a href="https://github.com/python/cpython/pull/31787"><u>dictionary watchers</u></a>, <a href="https://github.com/python/cpython/pull/97875"><u>type watchers</u></a>, <a href="https://github.com/python/cpython/pull/98175"><u>function watchers</u></a>, and <a href="https://github.com/python/cpython/pull/99859"><u>code object watchers</u></a>. All of these allow the Cinder JIT to be notified of dynamic changes that might invalidate its assumptions, so its fast path can remain as fast as possible.</li>
  <li>We landed <a href="https://github.com/python/cpython/pull/102022"><u>extensibility in the code generator for CPython’s core interpreter</u></a> that will allow Static Python to easily re-generate an interpreter with added Static Python opcodes, and a <a href="https://github.com/python/cpython/pull/102014"><u>C API to visit all GC-tracked objects</u></a>, which will allow the Cinder JIT to discover functions that were created before it was enabled.</li>
  <li>We also added a <a href="https://github.com/python/cpython/pull/103546"><u>thread-safe API for writing to perf-map files</u></a>. Perf-map files allow the Linux perf profiler to give a human-readable name to dynamically-generated sections of machine code, e.g. from a JIT compiler. This API will allow the Cinder JIT to safely write to perf map files without colliding with other JITs or with the new Python 3.12 <a href="https://github.com/python/cpython/pull/96123"><u>perf trampoline feature</u></a>.</li>
</ul>

<p>These improvements will be useful to anyone building a third party JIT compiler or runtime optimizer for CPython.
There are also plans to use the watchers internally in core CPython.</p>

<h1 id="beyond-python-312">Beyond Python 3.12</h1>

<p>Python plays a significant role at Meta.
It’s an important part of our infrastructure, including the <a href="https://engineering.fb.com/2023/10/05/developer-tools/python-312-meta-new-features/">Instagram server stack</a>.
And it’s the lingua franca for <a href="https://ai.meta.com/blog/code-llama-large-language-model-coding/">our AI/ML work</a>, highlighted by our development of <a href="https://pytorch.org/">PyTorch</a>, a machine learning framework for a wide range of use cases including computer vision, natural language processing, and more.</p>

<p>Our work with the Python community doesn’t end with the 3.12 release.
We are currently discussing a new proposal, <a href="https://peps.python.org/pep-0703/">PEP 703</a>, with the Python Steering Council to remove the GIL and allow Python to run in multiple threads in parallel.
This update could greatly help anyone using Python in a multi-threaded environment.</p>

<p>Meta’s involvement with the Python community also goes beyond code.
In 2023, we continued supporting the <a href="https://pyfound.blogspot.com/2022/03/meta-deepens-its-investment-in-python.html">Developer in Residence program for Python</a> and sponsored events like <a href="https://us.pycon.org/2023/#">PyCon US</a>.
We also shared our learnings in talks like “<a href="https://us.pycon.org/2023/schedule/presentation/155/">Breaking Boundaries: Advancements in High-Performance AI/ML through PyTorch’s Python Compiler</a>” and posts on the <a href="https://engineering.fb.com/?s=python">Meta Engineering blog</a>.</p>

<p>We are grateful to be a part of this open source community and look forward to working together to move the Python programming language forward.</p>

<hr />

<p>References</p>

<ul>
  <li><a href="https://engineering.fb.com/2023/10/05/developer-tools/python-312-meta-new-features/">Meta contributes new features to Python 3.12 - Engineering at Meta</a></li>
</ul>]]></content><author><name>miintto</name></author><category term="meta engineering" /><category term="python" /><category term="immortal object" /><category term="asyncio" /><summary type="html"><![CDATA[이번 파이썬 3.12 릴리즈에는 우리 내부의 사용 사례를 파이썬 커뮤니티에서 더 쉽게 접근할 수 있도록 개발 및 확장하였던 메타의 노력이 담겨있습니다. 이처럼 메타를 구성하는 오픈소스는 우리의 작업 방식과 학습한 결과를 커뮤니티에 공유하는 아주 중요한 역할을 합니다.]]></summary></entry><entry><title type="html">Copy-on-Write 이란?</title><link href="https://miintto.github.io/docs/os-cow" rel="alternate" type="text/html" title="Copy-on-Write 이란?" /><published>2024-04-21T00:00:00+00:00</published><updated>2024-04-21T00:00:00+00:00</updated><id>https://miintto.github.io/docs/os-cow</id><content type="html" xml:base="https://miintto.github.io/docs/os-cow"><![CDATA[<p>지난번 인스타그램이 서버를 최적화한 여러 포스트를 작성하면서 ‘Copy on Write’ 라는 키워드를 많이 접하게 되었습니다.
관련해서 찾아보았던 내용 아래에 정리해 보았습니다.</p>

<hr />

<h1 id="1-virtual-memory">1. Virtual Memory</h1>

<p>운영체제는 프로세스 실행 시에 고유한 메모리를 할당하게 되는데, 실제 컴퓨터의 물리 메모리가 아니라 추상화된 자원을 할당합니다.
이러한 추상화된 메모리 공간을 <strong>가상 메모리</strong>(Virtual Memory)라고 합니다.</p>

<p>가상 메모리는 컴퓨터의 메모리보다 큰 프로세스를 실행하기 위해 고안된 기술입니다.
여러 프로세스가 동시에 실행되는 멀티프로세싱 환경에서는 프로세스가 점유하는 메모리가 컴퓨터의 실제 물리 메모리를 초과하는 경우가 발생할 수 있는데, 이러한 상황에서 자원을 효율적으로 관리하기 위해 가상 메모리라는 개념이 도입되었습니다.</p>

<p>운영체제는 가상 메모리 주소(virtual address) 공간을 할당하고, 필요할 때마다 실제 물리 주소(physical address)에 매핑합니다.
이러한 작업은 운영체제에서 <strong>MMU</strong>(Memory Management Unit)라고 불리는 메모리 관리 유닛에 의해 처리됩니다.</p>

<p>이렇게 가상 메모리를 도입하면서 여러 이점이 있는데, 우선 응용 프로그램이 공유 메모리 공간을 직접 관리할 필요가 없어졌습니다.
또한 프로세스 간의 공유 메모리를 활용하여 공통으로 사용하는 라이브러리를 효율적으로 관리할 수 있습니다.
그리고 페이징이나 세그먼트 기술로 컴퓨터가 보유한 메모리보다 더 많은 메모리를 할당하여 프로그램을 실행할 수 있습니다.</p>

<h2 id="11-page">1.1 Page</h2>

<p>가상 메모리 구현체는 메모리 공간을 연속적인 가상 메모리 주소 블록으로 나누어 저장하는데, 이러한 블록을 <strong>페이지</strong>(Page)라고 합니다.
하나의 페이지는 최소 4KB의 크기를 가집니다.</p>

<p><img src="/img/posts/os-cow-page-table.png" style="max-width:540px" /></p>

<p><strong>페이지 테이블</strong>(Page Table)은 메모리의 가상 주소와 물리 주소를 연결하는 자료 구조입니다.
응용 프로그램은 가상 주소 공간에서 작동하지만 페이지 테이블에 따라 물리 주소로 변환되어 메모리에 접근하게 됩니다.
이러한 변환을 처리하는 하드웨어가 바로 위에서 설명한 MMU입니다.</p>

<p>각 페이지 테이블 엔트리에는 해당 페이지가 실제 물리 메모리에 있는지 여부를 추적하고 있습니다.
해당 페이지가 메모리에 존재하는 경우에는 물리 주소를 포함하고, 존재하지 않는 경우에는 디스크로 이동해야 함을 알리는 플래그를 포함하고 있습니다.</p>

<p>가상 메모리의 모든 데이터가 물리 메모리에 저장되는 것은 아닙니다.
메모리가 부족해짐에 따라 일부 데이터가 물리 메모리에서 디스크로 옮겨질 수 있습니다.
이때 물리 메모리에 존재하지 않는 페이지에 접근하는 경우 <strong>페이지 폴트</strong>(Page Fault)를 발생시키고 해당 페이지를 디스크에서 메모리로 옮겨오거나 다른 필요한 조치를 취하게 됩니다.</p>

<hr />

<h1 id="2-copy-on-write">2. Copy on Write</h1>

<p><strong>Copy on Write</strong>(COW)는 여러 프로세스가 물리적인 메모리 공간을 공유하는 경우에 이를 최적화 하기 위한 작업입니다.
메모리 페이지에 대해서 복제본이 필요한 경우에만 복사를 수행하여 공유 메모리를 효율적으로 관리합니다.</p>

<p>COW는 다음 방식으로 작동합니다.</p>

<ol>
  <li>먼저 메모리의 특정 페이지를 읽기 전용으로 표시합니다.</li>
  <li>읽기 전용 페이지에 대한 변경 요청이 들어오면 커널이 새로운 물리 공간을 할당합니다. 만일 해당 페이지에 대한 참조 개수가 하나뿐이라면 새로운 할당 없이 기존 페이지에 직접 데이터를 변경합니다.</li>
  <li>페이지 테이블의 참조를 업데이트하고 변경할 데이터를 입력합니다.</li>
</ol>

<p>이런 방식으로 원본 데이터를 보호하면서 이루어진 변경이 다른 프로세스에는 영향이 가지 않도록 할 수 있습니다.</p>

<p>COW는 보통 fork() 시스템 호출 작업을 수행하면서 많이 발생합니다.</p>

<p><img src="/img/posts/os-cow-process-fork.png" style="max-width:480px" /></p>

<p>기본적으로 fork() 시스템 호출로 자식 프로세스를 생성하면 부모 프로세스의 메모리가 그대로 복제됩니다.
이때 각 프로세스는 독립된 가상 주소 공간을 가지지만 동일한 물리 메모리를 바라보게 됩니다.</p>

<p><img src="/img/posts/os-cow-process-cow.png" style="max-width:480px" /></p>

<p>이때 부모 프로세스의 모든 메모리를 복제하게 된다면 비효율적이기 때문에 COW 메커니즘을 통해 데이터가 수정되는 경우에만 완전한 복제를 수행합니다.</p>

<p>Copy-on-Write는 프로세스 생성 초기에 메모리 복사와 관련된 오버헤드를 줄여줌으로써 성능을 향상할 수 있습니다.
특히 메모리 자원을 공유하는 프로세스가 많을수록 더 효율적입니다.
다만 메모리의 수정이 자주 발생하는 경우엔 이러한 이점이 감소할 수 있으며, 특히 메모리 페이지에 대한 복사본이 필요한 순간까지 메모리를 공유하고 있어야 하므로 추가적인 메모리 사용이 발생할 수 있습니다.</p>

<hr />

<p>References</p>

<ul>
  <li><a href="https://en.wikipedia.org/wiki/Virtual_memory">Virtual memory - Wikipedia</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Copy-on-write">Copy-on-write - Wikipedia</a></li>
  <li><a href="https://www.cs.uic.edu/%7Ejbell/CourseNotes/OperatingSystems/9_VirtualMemory.html">Operating Systems: Virtual Memory</a></li>
  <li><a href="https://taepcsiandwe.tistory.com/entry/%ED%8E%98%EC%9D%B4%EC%A7%80-%ED%85%8C%EC%9D%B4%EB%B8%94">페이지 테이블 :: 과학을 이해하는 개발자</a></li>
  <li><a href="https://talkingaboutme.tistory.com/entry/Study-Copy-On-Write-COW">[Study] Copy On Write (COW)</a></li>
  <li><a href="https://blog.naver.com/sqlmvp/140192006270">가상 메모리 쓰기 시 복사 (copy-on-write) : 네이버블로그</a></li>
</ul>]]></content><author><name>miintto</name></author><category term="operating system" /><category term="virtual memory" /><category term="copy-on-write" /><summary type="html"><![CDATA[지난번 인스타그램이 서버를 최적화한 여러 포스트를 작성하면서 ‘Copy on Write’ 라는 키워드를 많이 접하게 되었습니다. 관련해서 찾아보았던 내용 아래에 정리해 보았습니다.]]></summary></entry><entry><title type="html">운영체제에서 프로세스간 통신</title><link href="https://miintto.github.io/docs/os-ipc" rel="alternate" type="text/html" title="운영체제에서 프로세스간 통신" /><published>2024-04-10T00:00:00+00:00</published><updated>2024-04-10T00:00:00+00:00</updated><id>https://miintto.github.io/docs/os-ipc</id><content type="html" xml:base="https://miintto.github.io/docs/os-ipc"><![CDATA[<h1 id="1-inter-process-communication">1. Inter Process Communication</h1>

<p>운영체제에서 각 프로세스는 고유한 메모리가 할당되어 서로 독립적으로 실행됩니다.
하지만 여러 프로세스가 동시에 실행되는 멀티프로세스 환경에서는 다른 프로세스와 데이터를 주고받아야 하는 상황이 발생할 수 있습니다.
이를 위해 운영체제에는 프로세스 간의 데이터 전송, 동기화 등을 담당하는 <strong>IPC</strong>(Inter Process Communication)라는 메커니즘이 존재합니다.</p>

<p>IPC는 소프트웨어의 성능, 모듈화, 시스템 상황 등 여러 요구사항과 맞물려 다양한 형태로 구현되었습니다.</p>

<hr />

<h1 id="2-file">2. File</h1>

<p><strong>파일</strong>(File)을 이용한 IPC 메커니즘은 파일 시스템을 사용하여 여러 프로세스가 공유 파일에 데이터를 쓰고 읽는 방식으로 동작합니다.</p>

<p><img src="/img/posts/os-ipc-file.png" style="max-width:420px" /></p>

<p>언뜻 보면 간단해 보이지만 여러 제약사항 때문에 현재는 잘 사용하지 않고 있습니다.</p>

<p>우선 여러 프로세스가 동시에 하나의 파일에 접근하는 경우 동기화 이슈가 발생할 수 있는데, 이를 위해 mutex, semaphore, file locking 등과 같은 추가적인 메커니즘이 필요합니다.
또한 파일로 데이터를 주고받기 때문에 파일을 열고 닫고 데이터를 읽고 쓰는 과정에서 오버헤드가 발생할 수 있어서 다른 IPC 방식보다 속도가 느릴 수 있습니다.
보안 이슈도 발생할 수 있는데 파일 시스템에 데이터를 저장하기 때문에 공유 파일에 대한 접근 권한만 있으면 누구나 데이터를 확인할 수 있다는 문제가 있습니다.</p>

<p>이러한 제약사항 때문에 간단한 통신이나 데이터의 규모가 작은 경우에는 파일을 이용한 통신이 적합할 수 있지만, 대부분의 상황에서는 다른 IPC 메커니즘을 사용하는 게 더 효율적입니다.</p>

<hr />

<h1 id="3-signal">3. Signal</h1>

<p><strong>시그널</strong>(Signal)은 특정 이벤트를 프로세스로 전달하는 소프트웨어 신호입니다.
예를 들면 인터럽트 시그널(Ctrl+C)과 같은 사용자의 명령이나 하드웨어 인터럽트 신호를 특정 프로세스에 전달하여 해당하는 명령을 수행하도록 합니다.</p>

<p><img src="/img/posts/os-ipc-signal.png" style="max-width:420px" /></p>

<p>이벤트를 받은 프로세스가 수행할 작업은 시그널 핸들러라는 함수 내부에 정의할 수 있습니다.
그리고 signal() 혹은 sigaction() 와  같은 시스템 호출 함수를 통해 특정 시그널 수신 시에 실행될 시그널 핸들러를 설정할 수 있습니다.</p>

<p>본래 시그널이 IPC 때문에 만들어진 기능은 아니지만 프로세스간의 통신 메커니즘으로 사용할 수도 있습니다.
특히 <code class="language-plaintext highlighter-rouge">SIGUSR1</code>과 <code class="language-plaintext highlighter-rouge">SIGUSR2</code>와 같은 사용자 정의 시그널을 활용할 수 있는데, 프로세스가 해당 시그널에 대한 시그널 핸들러를 등록하여 사용자가 정의한 상황이 발생할 때마다 다른 프로세스에 이벤트를 전달할 수 있습니다.</p>

<p>이러한 시그널은 비동기적으로 전달되기 때문에 송수신 간의 동기화가 필요하지 않습니다.
따라서 간단한 구현이 가능하면서도 신속하게 메시지를 전달할 수 있습니다.
다만 시그널의 기본적인 목적이 특정한 이벤트를 전달이기 때문에 큰 데이터를 전달하 는데에는 적합하지 않습니다.
또한 추가적인 시그널 핸들러를 등록하고 관리해야 하는 번거로움이 생길 수 있습니다.</p>

<hr />

<h1 id="4-socket">4. Socket</h1>

<p>본래 네트워크 인터페이스는 프로세스뿐 아니라 다른 컴퓨터에도 데이터를 전송하도록 고안된 메커니즘입니다.
이러한 <strong>소켓</strong>(Socket) 통신을 로컬 프로세스간에 사용하도록 하여 IPC를 구현할 수 있습니다.</p>

<p>소켓을 사용하면 프로세스 간의 양방향 통신이 가능합니다.
또한 대용량의 데이터도 전달할 수 있습니다.
이때 신뢰성 있는 연결 지향 통신(TCP/IP)이나 비연결성의 빠른 통신(UDP) 중에서 요구사항에 적합한 프로토콜을 선택할 수도 있습니다.
또한 SSL/TLS 같은 보안 프로토콜을 사용한다면 안전한 통신을 보장할 수 있습니다.</p>

<p>자세한 소켓의 동작에 대해서는 <a href="/docs/unix-socket">해당 포스트</a>에 설명해 두었습니다.</p>

<hr />

<h1 id="5-message-queue">5. Message Queue</h1>

<p><strong>메시지 큐</strong>(Message Queue)를 사용하여 프로세스끼리 통신할 수도 있습니다.
메시지를 보내는 프로세스가 큐에 메시지를 입력하면 다른 프로세스가 큐에서 메시지를 읽어가는 방식으로 작동합니다.</p>

<p><img src="/img/posts/os-ipc-message-queue.png" style="max-width:420px" /></p>

<p>메시지 큐를 사용하면 직접 연결되지 않은 여러 프로세스끼리 비동기적으로 통신이 가능합니다.
또한 운영체제에서 기본적으로 제공하는 기능이라 안정성과 신뢰성이 보장됩니다.</p>

<hr />

<h1 id="6-shared-memory">6. Shared Memory</h1>

<p><strong>공유 메모리</strong>(Shared Memory) 방식은 여러 프로세스가 동일한 물리적인 메모리 공간에 접근하여 통신하는 방법입니다.
한 프로세스가 다른 프로세스도 접근 가능한 영역을 생성하고 이를 다른 프로세스에 공유하는 방식으로 작동합니다.
이렇게 하면 다른 프로세스는 공유 메모리에 직접 데이터를 읽고 쓸 수 있습니다.</p>

<p><img src="/img/posts/os-ipc-shared-memory.png" style="max-width:420px" /></p>

<p>여러 프로세스가 동일한 메모리 공간에 직접 접근하기 때문에 앞서 설명드린 IPC 기법과 비교했을 때 훨씬 빠르게 통신할 수 있습니다.
하지만 메모리 공간을 직접 공유하는 방식이라서 같은 호스트 내부의 프로세스끼리만 통신이 가능하여 확장성이 떨어질 수 있습니다.
또한 다수의 프로세스가 접근하는 경우 데이터의 동기화 이슈가 발생할 수 있습니다.</p>

<p>일반적으로 여러 프로세스가 동일한 라이브러리를 사용하는 경우 해당 라이브러리의 코드와 데이터는 메모리에 한 번만 불러오게 됩니다.
그리고 프로세스가 라이브러리의 특정 메모리 페이지를 수정하게 된다면 해당 수정 내용은 프로세스 개별적으로만 반영되어야 합니다.
여기서 수정이 필요한 페이지는 <a href="/docs/os-cow">Copy-on-Write</a> 메커니즘을 사용하여 처리되는데, 페이지에 수정 요청이 들어온 경우에 새로운 페이지를 할당하여 변경된 내용을 입력하게 됩니다.
이런 방법으로 여러 프로세스가 동일한 라이브러리를 공유하면서도 필요한 경우에만 페이지가 복제되어 독립성을 유지할 수 있습니다.</p>

<hr />

<p>References</p>

<ul>
  <li><a href="https://en.wikipedia.org/wiki/Inter-process_communication">Inter-process communication - Wikipedia</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Shared_memory">Shared memory - Wikipedia</a></li>
  <li><a href="https://devraphy.tistory.com/175">22. 프로세스 - IPC 기법(signal, socket)</a></li>
  <li><a href="https://dokhakdubini.tistory.com/490">IPC와 Shared Memory</a></li>
  <li><a href="https://hyunie-y.tistory.com/32">운영체제 6: 프로세스 간 커뮤니케이션 - 가상 메모리와 IPC에 대해</a></li>
</ul>]]></content><author><name>miintto</name></author><category term="operating system" /><category term="ipc" /><category term="process" /><summary type="html"><![CDATA[운영체제에서 각 프로세스는 고유한 메모리가 할당되어 서로 독립적으로 실행됩니다. 하지만 여러 프로세스가 동시에 실행되는 멀티프로세스 환경에서는 다른 프로세스와 데이터를 주고받아야 하는 상황이 발생할 수 있습니다. 이를 위해]]></summary></entry><entry><title type="html">[번역] Python에 불멸 객체 도입</title><link href="https://miintto.github.io/docs/meta-immortal-objects" rel="alternate" type="text/html" title="[번역] Python에 불멸 객체 도입" /><published>2024-03-14T00:00:00+00:00</published><updated>2024-03-14T00:00:00+00:00</updated><id>https://miintto.github.io/docs/meta-immortal-objects</id><content type="html" xml:base="https://miintto.github.io/docs/meta-immortal-objects"><![CDATA[<blockquote>
  <p>해당 포스트는 Meta Engineering 블로그의 <a href="https://engineering.fb.com/2023/08/15/developer-tools/immortal-objects-for-python-instagram-meta/">Introducing Immortal Objects for Python</a> 포스트를 번역한 글입니다.</p>

  <p>게시일: 2023.08.15</p>
</blockquote>

<h1 id="파이썬에-불멸-객체-도입">파이썬에 불멸 객체 도입</h1>

<p>Meta에서는 인스타그램 프론트엔드 서버로 파이썬(Django)을 사용하고 있습니다.
해당 환경에서 병렬 처리를 위해 프로세스마다 asyncio 기반의 동시성을 처리하는 멀티 프로세스 아키텍처로 운영하고 있습니다.
하지만 인스타그램 정도 규모의 비즈니스 로직과 서버 요청을 감당하기에는 이러한 구조가 메모리에 부담을 주어 병목지점이 생길 수 있습니다.</p>

<p>이러한 현상을 완화시키기 위해 우리는 가능한 많은 객체를 캐싱하는 pre-fork 웹서버와, 공유 메모리를 통해 읽기 전용 모드로 해당 객체에 접근하는 별도 프로세스들로 구성하였습니다.
이런 접근법이 어느 정도 도움은 되었지만, 면밀히 따져보았을 때 시간이 지날수록 공유 메모리 사용량이 줄어들고 반대로 private 메모리 사용량은 증가하는 현상이 발견되었습니다.</p>

<p>그래서 파이썬의 힙(heap) 메모리를 조사해 보았는데, 대다수 파이썬 객체가 실질적으로 불멸(immutable) 객체처럼 전체 런타임 실행 내내 살아있었지만, 레퍼런스 카운팅과 가비지 컬렉션 사이클 작업을 거치면서 이러한 객체에 대한 메타데이터 조정 작업이 수행되었고, 결국 서버 프로세스에 COW(Copy on Write)를 유발한다는 사실을 발견했습니다.</p>

<p><img src="/img/posts/meta-immortal-objects-img001.jpg" style="max-width:600px" />
<span class="caption text-muted">메인 프로세스에서 COW의 영향으로 private 메모리가 늘어나고 공유 메모리가 줄어드는 현상</span></p>

<h2 id="파이썬을-위한-불멸-객체">파이썬을 위한 불멸 객체</h2>

<p>사실 이런 공유 객체 간의 mutation 상태 이슈는 파이썬 런타임이 작동하는 핵심 구성요소입니다.
파이썬 런타임이 레퍼런스 카운팅과 GC 사이클에 의존하고 있기 때문에 객체의 핵심 메모리 구조를 건드릴 수 밖에 없고, 그 때문에 GIL도 불가피한 상황입니다.</p>

<p>우리는 이런 문제를 해결하기 위해 불멸 객체(<a href="https://peps.python.org/pep-0683/">PEP-683</a>)라는 개념을 도입했습니다.
파이썬 객체의 레퍼런스 카운트 필드에 특별한 값을 표기하면서 핵심 객체의 상태가 절대 변하지 않는 불멸 객체를 고안해 냈습니다.
그래서 런타임이 언제 레퍼런스 카운트 필드와 GC 헤더를 mutate 할 수 있는지 구별할 수 있게 되었습니다.</p>

<p><img src="/img/posts/meta-immortal-objects-img002.jpg" style="max-width:600px" />
<span class="caption text-muted">표준 객체와 불멸 객체간의 비교. 표준 객체 사용시 타입이나 데이터가 mutate하지 않도록 보장. 불멸성은 추가적으로 런타임이 레퍼런스 카운트나 GC 헤더를 수정하지 않음을 보장하여 전체 객체의 불변성을 활성화.</span></p>

<p>이런 방식을 인스타그램 내부에 적용하는 건 비교적 어렵지 않았지만, 커뮤니티에 발표하기까지 매우 길고 힘든 과정이 있었습니다.
가장 큰 문제는 솔루션의 구현 방식이었습니다.
해당 솔루션은 하위 호환성, 플랫폼 호환성, 성능 저하와 같이 얽혀 있는 복잡한 문제들을 모두 아우르는 해결책을 담아야만 합니다.</p>

<p>먼저, 레퍼런스 카운트 계산 방식이 변경된 후에 특정 객체의 refcount가 갑자기 달라진 상황에서도 프로그램이 크래시가 나지 않아야 합니다.</p>

<p>또한, 파이썬 객체의 핵심 메모리 구현체와 레퍼런스 카운트 증감 메커니즘이 변경되어야 합니다.
그러면서도 그동안 지원하던 모든 플랫폼(Unix, Window, Mac)과 컴파일러(GCC, Clang, MSVC), 아키텍쳐(32-bit, 64-bit), 하드웨어 타입(little-endian, big-endian)에서도 동작해야 합니다.</p>

<p>마지막으로, 해당 구현체의 핵심은 레퍼런스 카운트 증감 부분에 추가한 명시적인 검증 로직인데, 해당 경로는 런타임 실행 중에 가장 빈번하게 호출되는 부분이었습니다.
이에 따라 서비스의 성능 저하가 불가피해졌지만, 다행히도 레지스터 할당 부분을 잘 활용하면서 전반적인 시스템 성능이 고작 2% 하락에 그치도록 하였고, 그 대가로 얻는 성능적인 이익을 생각해 보면 합당한 수치였습니다.</p>

<h2 id="불멸-객체가-인스타그램에-미친-영향">불멸 객체가 인스타그램에 미친 영향</h2>

<p>다시 처음으로 돌아와서, 인스타그램의 초기 목표는 COW를 감소시켜서 매 요청마다 메모리와 CPU 효율성을 향상시키는 것이었습니다.
그리고 불멸 객체를 도입하면서 공유 메모리 사용량을 높이고 private 메모리 사용량을 줄일 수 있었습니다.</p>

<p><img src="/img/posts/meta-immortal-objects-img003.jpg" style="max-width:600px" />
<span class="caption text-muted">불멸 객체를 사용하면서 공유 메모리 사용량 증가 및 private 메모리 사용량의 현저한 감소. COW 횟수 감소</span></p>

<p>이러한 변화는 인스타그램 내부의 개선을 넘어서 파이썬 언어의 진화에도 영향을 주었습니다.
현재까지 파이썬의 제약사항 중 하나는 힙 공간에서 객체의 진정한 불변성을 보장할 수 없다는 것이었습니다.
GC와 레퍼런스 카운트 메커니즘 하에서 객체의 레퍼런스 카운트 필드와 GC 헤더에 무제한 접근할 수 있었습니다.</p>

<p>하지만 파이썬에 불멸 객체를 도입하면서 최초로 진정한 불변성을 보장하였습니다.
이에 따라 특정 객체는 레퍼런스 카운팅과 가비지 컬렉션 대상에서 제외될 수 있습니다.
즉, 굳이 GIL 없이도 불멸 객체를 스레드 간에 공유할 수 있게 되었습니다.</p>

<p>이런 방식은 multi-core 파이썬을 향한 중요한 기반이 될 수 있습니다.
이를 위해 불멸 객체를 활용하는 방법으로 아래 두 가지 제안이 있습니다.</p>

<ul>
  <li><a href="https://peps.python.org/pep-0684/">PEP-684</a>: 인터프리터마다 GIL 설정</li>
  <li><a href="https://peps.python.org/pep-0703/">PEP-703</a>: CPython에서 GIL을 선택 사항으로 만들기</li>
</ul>

<h2 id="오늘날-불멸-객체">오늘날 불멸 객체</h2>

<p>우리는 커뮤니티와 함께 이러한 ‘불멸화’의 개념을 서로의 시스템에서 잘 활용하는 방법을 고민해 보고, 기존 제안을 살펴보면서 multi-core 환경에서 어플리케이션을 개선할 방안을 기대하고 있습니다.
Meta에서는 이러한 언어의 발전 방향에 흥미를 느끼고 있으며, 인스타그램을 연구하고 발전시키면서도 회사 외적으로 기여할 준비가 되어있습니다.</p>

<hr />

<h1 id="introducing-immortal-objects-for-python">Introducing Immortal Objects for Python</h1>

<p>At Meta, we use Python (Django) for our frontend server within Instagram.
To handle parallelism, we rely on a multi-process architecture along with asyncio for per-process concurrency.
However, our scale – both in terms of business logic and the volume of handled requests –  can cause an increase in memory pressure, leading to efficiency bottlenecks.</p>

<p>To mitigate this effect, we rely on a pre-fork web server architecture to cache as many objects as possible and have each separate process use them as read-only structured through shared memory.
While this greatly helps, upon closer inspection we saw that our processes’ private memory usage grew over time while our shared memory decreased.</p>

<p>By analyzing the Python heap, we found that while most of our Python Objects were practically immutable and lived throughout the entire execution of the runtime, it ended up still modifying these objects through reference counts and garbage collection (GC) operations that mutate the objects’ metadata on every read and GC cycle –  thus, triggering a copy on write on the server process.</p>

<p><img src="/img/posts/meta-immortal-objects-img001.jpg" style="max-width:600px" />
<span class="caption text-muted">The effect of copy on writes is increasing private memory and a reduction of shared memory from the main process.</span></p>

<h2 id="immortal-objects-for-python">Immortal Objects for Python</h2>

<p>This problem of state mutation of shared objects is at the heart of how the Python runtime works.
Given that it relies on reference counting and cycle detection, the runtime requires modifying the core memory structure of the object, which is one of the reasons the language requires a global interpreter lock (GIL).</p>

<p>To get around this issue, we introduced Immortal Objects – PEP-683.
This creates an immortal object (an object for which the core object state will never change) by marking a special value in the object’s reference count field.
It allows the runtime to know when it can and can’t mutate both the reference count fields and GC header.</p>

<p><img src="/img/posts/meta-immortal-objects-img002.jpg" style="max-width:600px" />
<span class="caption text-muted">A comparison of standard objects versus immortal objects. With standard objects, a user can guarantee that it will not mutate its type and/or its data. Immortality adds an extra guarantee that the runtime will not modify the reference count or the GC Header if present, enabling full object immutability.</span></p>

<p>While implementing and releasing this within Instagram was a relatively straightforward process due to our relatively isolated environment, sharing this to the community was a long and arduous process.
Most of this was due to the solution’s implementation, which had to deal with a combination of problems such as backwards compatibility, platform compatibility, and performance degradation.</p>

<p>First, the implementation had to guarantee that, even after changing the reference count implementation, applications wouldn’t crash if some objects suddenly had different refcount values.</p>

<p>Second, it changes the core memory representation of a Python object and how it increases its reference counts.
It needed to work across all the different platforms (Unix, Windows, Mac), compilers (GCC, Clang, and MSVC), architectures (32-bit and 64-bit), and hardware types (little- and big-endian).</p>

<p>Finally, the core implementation relies on adding explicit checks in the reference count increment and decrement routines, which are two of the hottest code paths in the entire execution of the runtime.
This inevitably meant a performance degradation in the service.
Fortunately, with the smart usage of register allocations, we managed to get this down to just a ~2 percent regression across every system, making it a reasonable regression for the benefits that it brings.</p>

<h2 id="how-immortal-objects-have-impacted-instagram">How Immortal Objects have impacted Instagram</h2>

<p>For Instagram, our initial focus was to achieve improvements in both memory and CPU efficiency of handling our requests by reducing copy on writes.
Through immortal objects, we managed to greatly reduce private memory by increasing shared memory usage.</p>

<p><img src="/img/posts/meta-immortal-objects-img003.jpg" style="max-width:600px" />
<span class="caption text-muted">Increasing shared memory usage through immortal Objects allows us to significantly reduce private memory. Reducing the number of copy on writes.</span></p>

<p>However, the implications of these changes go far beyond Instagram and into the evolution of Python as a language.
Until now, one of Python’s limitations has been that it couldn’t guarantee true immutability of objects on the heap.
Both the GC and the reference count mechanism had unrestricted access to both of these fields.</p>

<p>Contributing immortal objects into Python introduces true immutability guarantees for the first time ever.
It helps objects bypass both reference counts and garbage collection checks.
This means that we can now share immortal objects across threads without requiring the GIL to provide thread safety.</p>

<p>This is an important building block towards a multi-core Python runtime.
There are two proposals that leverage immortal objects to achieve this in different ways:</p>

<ul>
  <li><a href="https://peps.python.org/pep-0684/">PEP-684</a>: A Per-Interpreter GIL</li>
  <li><a href="https://peps.python.org/pep-0703/">PEP-703</a>: Making the Global Interpreter Lock Optional in CPython</li>
</ul>

<h1 id="try-immortal-objects-today">Try Immortal Objects today</h1>

<p>We invite the community to think of ways they can leverage immortalization in their applications as well as review the existing proposals to anticipate how to improve their applications for a multi-core environment.
At Meta, we are excited about the direction in the language’s development and we are ready to keep contributing externally while we keep experimenting and evolving Instagram.</p>

<hr />

<p>References</p>

<ul>
  <li><a href="https://engineering.fb.com/2023/08/15/developer-tools/immortal-objects-for-python-instagram-meta/">Introducing Immortal Objects for Python - Engineering at Meta</a></li>
</ul>]]></content><author><name>miintto</name></author><category term="meta engineering" /><category term="python" /><category term="immortal object" /><category term="instagram" /><summary type="html"><![CDATA[Meta에서는 인스타그램 프론트엔드 서버로 파이썬(Django)을 사용하고 있습니다. 해당 환경에서 병렬 처리를 위해 프로세스마다 asyncio 기반의 동시성을 처리하는 멀티 프로세스 아키텍처로 운영하고 있습니다.]]></summary></entry><entry><title type="html">[알고리즘] 타일링</title><link href="https://miintto.github.io/docs/tiling" rel="alternate" type="text/html" title="[알고리즘] 타일링" /><published>2024-03-06T00:00:00+00:00</published><updated>2024-03-06T00:00:00+00:00</updated><id>https://miintto.github.io/docs/tiling</id><content type="html" xml:base="https://miintto.github.io/docs/tiling"><![CDATA[<p>제가 지금까지 개발자로 커리어를 쌓아 왔지만 사실 저의 전공은 수학이었습니다.
학부생 시절 수강했던 강의 중 인상깊었던 과목 중 하나로 조합론 수업을 꼽을 수 있는데요.
현재까지도 개발자로 일하며 알고리즘 관련해서는 해당 수업의 영향을 크게 받고 있다고 생각합니다.</p>

<hr />

<h1 id="1-tiling">1. Tiling</h1>

<p>수학에서 <strong>타일링</strong>(tiling)이란 다차원의 표면을 작은 조각들로 덮는 방법을 말합니다. 
여기서는 길이 n의 직사각형 막대를 일정한 크기의 직사각형 타일로 채우는 경우만을 다루려고 합니다.</p>

<h2 id="11-fibonacci-number">1.1 Fibonacci Number</h2>

<p>먼저 길이가 n인 막대를 1x1, 1x2 크기를 가진 타일로 채우는 상황을 생각해봅시다.
예를 들어 n=4인 경우 아래 5가지 경우의 수가 있습니다.</p>

<p><img src="/img/posts/algorithm-tiling-example.png" style="max-width:540px" /></p>

<p>이때, 모든 경우의 수는 반드시 첫번째 타일의 길이가 1 혹은 2로 시작하는 두 가지 경우로 나누어집니다.
먼저 첫번째 타일의 길이가 1인 경우는 3가지인데 이는 뒤의 길이 3의 막대를 채우는 경우의 수와 동일합니다.
마찬가지로 첫번째 타일의 길이가 2인 경우는 나머지 길이 2의 막대를 채우는 경우의 수 2와 동일합니다.</p>

<p><img src="/img/posts/algorithm-tiling-divide.png" style="max-width:540px" /></p>

<p>이처럼 길이 n-1, n-2의 타일을 채우는 경우의 수를 알 수 있으면 해당 두 값을 더하는것 만으로 길이가 n인 타일의 경우의 수를 구할 수 있습니다.
길이가 n인 막대를 채우는 경우의 수를 f<sub>n</sub> 이라고 할 때 아래와 같은 공식이 성립합니다.</p>

<blockquote>
  <p>f<sub>n</sub> = f<sub>n-1</sub> + f<sub>n-2</sub></p>
</blockquote>

<p>아마 위와 같은 등식을 보고 <strong>피보나치 수열</strong>을 떠올릴 수 있을겁니다.
f<sub>0</sub> = 1, f<sub>-1</sub> = 0이라고 하면 f<sub>n</sub>은 완전히 피보나치 수열을 따르게 됩니다.
피보나치 수열의 일반항을 F<sub>n</sub>라고 할 때 아래 등식이 성립합니다.</p>

<blockquote>
  <p>f<sub>n</sub> = F<sub>n+1</sub></p>
</blockquote>

<table>
  <thead>
    <tr>
      <th>n=1</th>
      <th>n=2</th>
      <th>n=3</th>
      <th>n=4</th>
      <th>n=5</th>
      <th>n=6</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>11<br />2</td>
      <td>111<br />12<br />21</td>
      <td>1111<br />112<br />121<br />211<br />22</td>
      <td>11111<br />1112<br />1121<br />1211<br />2111<br />122<br />212<br />221</td>
      <td>111111<br />11112<br />11121<br />11211<br />12111<br />21111<br />1122<br />1212<br />1221<br />2112<br />2121<br />2211<br />222</td>
    </tr>
    <tr>
      <td>f<sub>1</sub> = 1</td>
      <td>f<sub>2</sub> = 2</td>
      <td>f<sub>3</sub> = 3</td>
      <td>f<sub>4</sub> = 5</td>
      <td>f<sub>5</sub> = 8</td>
      <td>f<sub>6</sub> =13</td>
    </tr>
  </tbody>
</table>

<h2 id="12-identites">1.2 Identites</h2>

<p>위에서 정의한 f<sub>n</sub>을 바탕으로 아래 등식을 증명할 수 있습니다.</p>

<blockquote>
  <p>f<sub>m+n</sub> = f<sub>m</sub>f<sub>n</sub> + f<sub>m-1</sub>f<sub>n-1</sub> (for m, n ≥ 0)</p>
</blockquote>

<p>흔히 수학에서 증명이라 함은 수식적으로 풀어서 해결하였지만, 조합론에서는 양 변의 타일링을 가정하여 서로 완전히 대응되는지를 확인합니다.</p>

<p>먼저 좌변은 m+n 타일링으로 가정합니다.
우변의 경우는 m+n 타일링을 두가지 케이스로 나누어 생각할 수 있는데, m번째 위치에서 나눌 수 있는지 아니면 나누어지지 않는지 입니다.
만일 m번째 위치에서 나눌 수 있다면 각 길이가 m, n인 두 개의 막대로 나눌 수 있고 이때의 경우의 수는 f<sub>m</sub>f<sub>n</sub>입니다.
길이가 2인 타일이 m, m+1에 위치하여 m번째에서 나눌 수 없다면 길이가 m-1, n-1인 두 개의 막대로 나눌 수 있습니다. 이때의 경우의 수는 f<sub>m-1</sub>f<sub>n-1</sub>입니다.</p>

<p><img src="/img/posts/algorithm-tiling-identities-mn.png" style="max-width:480px" /></p>

<p>이때 발생한 두 가지 경우의 수 f<sub>m</sub>f<sub>n</sub>와 f<sub>m-1</sub>f<sub>n-1</sub>를 더하면 완벽하게 m+n 타일링에 대응됩니다.</p>

<hr />

<h1 id="2-dynamic-programing">2. Dynamic Programing</h1>

<p>위에서 사용한 내용은 주로 알고리즘에서 Dynamic Programing의 형태로 나타납니다.
프로그래머스나 백준, leetcode 등에서도 해당 타일링을 응용한 문제를 어렵지 않게 발견할 수 있습니다.</p>

<h2 id="21-2--n-타일링---프로그래머스">2.1 [2 × n 타일링] - 프로그래머스</h2>

<p><a href="https://school.programmers.co.kr/learn/courses/30/lessons/12900">바로가기</a></p>

<p><img src="/img/posts/algorithm-tiling-programers-tiling.png" style="max-width:660px; box-shadow: 3px 5px 10px #CCC" /></p>

<p>타일을 채우는 방식이 세로로 하나씩 채우거나 가로 방향으로 두 개를 채우는 방법밖에 없는데, 사실상 위에서 알아본 1xn크기의 막대를 채우는 타일링과 완벽히 일치합니다.</p>

<p><img src="/img/posts/algorithm-tiling-example-2n.png" style="max-width:540px" /></p>

<p>이런 방식을 구현해보면 파이썬을 사용한 경우 아래와 같이 작성할 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">solution</span><span class="p">(</span><span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="p">)</span> <span class="o">%</span> <span class="mi">1000000007</span>
    <span class="k">return</span> <span class="n">b</span>
</code></pre></div></div>

<h2 id="22-타일링---백준-1793">2.2 [타일링] - 백준 1793</h2>

<p><a href="https://www.acmicpc.net/problem/1793">바로가기</a></p>

<p><img src="/img/posts/algorithm-tiling-baekjoon-1793.png" style="max-width:660px; box-shadow: 3px 5px 10px #CCC" /></p>

<p>해당 경우는 아래와 같은 세 가지 경우로 나누어 생각해 볼 수 있습니다.</p>

<p><img src="/img/posts/algorithm-tiling-example-2n-2.png" style="max-width:540px" /></p>

<p>첫번째 타일은 반드시 다음 세가지 중 하나입니다.</p>

<ul>
  <li>2x1 타일을 세로로 두거나,</li>
  <li>2x1 타일 두 개를 가로롤 채우거나</li>
  <li>2x2 타일을 채우거나</li>
</ul>

<p>이때 길이가 n인 직사각형을 채우는 경우의 수를 f<sub>n</sub>이라고 할 때 나머지 직사각형을 채우는 경우의 수는 각각 f<sub>n-1</sub>, f<sub>n-2</sub>, f<sub>n-2</sub> 입니다.
따라서 아래와 같은 공식이 성립합니다.</p>

<blockquote>
  <p>f<sub>n</sub> = 2f<sub>n-1</sub> + f<sub>n-2</sub></p>
</blockquote>

<p>해당 내용을 바탕으로 결과를 작성해보면 아래와 같습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">solution</span><span class="p">(</span><span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
    <span class="k">return</span> <span class="n">b</span>
</code></pre></div></div>

<hr />

<p>References</p>
<ul>
  <li>Arthur T. Benjamin and Jennifer J. Quinn, Proofs that Really Count: the Art of Combinatorial Proof, Mathematical Association of America, 2003-07</li>
</ul>]]></content><author><name>miintto</name></author><category term="algorithm" /><category term="tiling" /><category term="combinatorics" /><summary type="html"><![CDATA[제가 지금까지 개발자로 커리어를 쌓아 왔지만 사실 저의 전공은 수학이었습니다. 학부생 시절 수강했던 강의 중 인상깊었던 과목 중 하나로 조합론 수업을 꼽을 수 있는데요. 현재까지도 개발자로 일하며 알고리즘 관련해서는 해당 수업의 영향을 크게 받고 있다고 생각합니다.]]></summary></entry></feed>